{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4488b10d",
   "metadata": {},
   "source": [
    "# League of Legends – Model Research Notebook\n",
    "\n",
    "AI-driven predictive modeling to estimate the probability that the blue team wins a ranked match (0–1). This notebook evaluates four candidate models and compares their performance, following a What → How → Why structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a0316",
   "metadata": {},
   "source": [
    "## Problem, data, models, and metrics\n",
    "\n",
    "- Problem: Predict match outcome probability for League of Legends ranked games from early-game state (around 15 minutes) and pregame info.\n",
    "- Data: Cleaned 15-minute dataset prepared in `data/cleaned/` (kept separate from pregame).\n",
    "- Target: `blue_win` (1=blue wins, 0=blue loses).\n",
    "- Models to evaluate:\n",
    "  1. Logistic Regression (Baseline, classification) [R1]\n",
    "  2. Random Forest Regressor (Main, regression → threshold for metrics) [R1]\n",
    "  3. K-Nearest Neighbors (classification) [R1]\n",
    "  4. Linear Regression (regression → threshold for metrics) [R1]\n",
    "- Metrics: Accuracy, Precision, Recall, F1 on the test set (computed using a 0.5 threshold for probability/score outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7152aa",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "- H1: A non-linear model (Random Forest) will outperform linear baselines due to complex interactions (gold, objectives, lane stats).\n",
    "- H2: Logistic Regression will form a strong, interpretable baseline but may underfit non-linearities.\n",
    "- H3: KNN will underperform with many numeric features and mixed scales, even with standardization.\n",
    "- H4: Linear Regression can provide a quick regression baseline but will be less suitable for classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87541d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How: Imports and setup\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d1a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed splits from data/cleaned.\n",
      "train: (698, 20)\n",
      "val: (150, 20)\n",
      "test: (150, 20)\n"
     ]
    }
   ],
   "source": [
    "# How: Load cleaned dataset or splits from data/cleaned\n",
    "cleaned_dir = os.path.join('data', 'cleaned')\n",
    "fifteen_clean_path = os.path.join(cleaned_dir, 'lol_15min_data_clean.parquet')\n",
    "fifteen_clean_csv = os.path.join(cleaned_dir, 'lol_15min_data_clean.csv')\n",
    "train_path = os.path.join(cleaned_dir, 'lol_15min_train.parquet')\n",
    "val_path = os.path.join(cleaned_dir, 'lol_15min_val.parquet')\n",
    "test_path = os.path.join(cleaned_dir, 'lol_15min_test.parquet')\n",
    "\n",
    "def read_any(path_parquet: str, path_csv: str) -> pd.DataFrame:\n",
    "    if os.path.exists(path_parquet):\n",
    "        try:\n",
    "            return pd.read_parquet(path_parquet)\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f'Failed to read {path_parquet} ({exc}); will try CSV.')\n",
    "    if os.path.exists(path_csv):\n",
    "        try:\n",
    "            return pd.read_csv(path_csv)\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(f'Could not read {path_csv}: {exc}') from exc\n",
    "    raise FileNotFoundError(f'No dataset available at {path_parquet} or {path_csv}')\n",
    "\n",
    "# Prefer precomputed splits if present, otherwise load full clean dataset and split\n",
    "if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "    try:\n",
    "        train_df = pd.read_parquet(train_path)\n",
    "        val_df = pd.read_parquet(val_path) if os.path.exists(val_path) else None\n",
    "        test_df = pd.read_parquet(test_path)\n",
    "        print('Loaded precomputed splits from data/cleaned.')\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f'Split parquet read failed ({exc}); reverting to full dataset split.')\n",
    "        full_df = read_any(fifteen_clean_path, fifteen_clean_csv)\n",
    "        stratify_col = full_df['blue_win'] if 'blue_win' in full_df.columns else None\n",
    "        train_df, test_df = train_test_split(\n",
    "            full_df, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_col\n",
    "        )\n",
    "        val_df = None\n",
    "else:\n",
    "    full_df = read_any(fifteen_clean_path, fifteen_clean_csv)\n",
    "    stratify_col = full_df['blue_win'] if 'blue_win' in full_df.columns else None\n",
    "    train_df, test_df = train_test_split(\n",
    "        full_df, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_col\n",
    "    )\n",
    "    val_df = None\n",
    "\n",
    "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "    shape = 'None' if df is None else df.shape\n",
    "    print(f'{name}: {shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9326534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((698, 14), (150, 14))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How: Feature/label selection and minimal cleaning\n",
    "LABEL_COL = 'blue_win'\n",
    "ID_LIKE = {'matchId', 'gameId', 'match_id'}\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    assert LABEL_COL in df.columns, f'Missing label column: {LABEL_COL}'\n",
    "    y = df[LABEL_COL].astype(int).clip(0, 1)\n",
    "    # pick numeric features excluding label and ids\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c != LABEL_COL]\n",
    "    # include objective/categorical encodings if present and numeric\n",
    "    drop_cols = [c for c in df.columns if c in ID_LIKE]\n",
    "    X = df.drop(columns=drop_cols, errors='ignore')[num_cols].copy()\n",
    "    # fill remaining NaNs in numeric features\n",
    "    X = X.fillna(0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_xy(train_df)\n",
    "X_test, y_test = prepare_xy(test_df)\n",
    "X_val, y_val = (prepare_xy(val_df) if val_df is not None else (None, None))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d8d2c",
   "metadata": {},
   "source": [
    "## Train and evaluate models\n",
    "We train the four specified models and evaluate Accuracy, Precision, Recall, and F1 on the test set. For regressors (Random Forest Regressor, Linear Regression), we convert predictions to class labels via a 0.5 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c715ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to evaluate a model (classifier with predict_proba or regressor with continuous output)\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te, kind: str, threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    fitted = model.fit(X_tr, y_tr)\n",
    "    if kind == 'classifier':\n",
    "        if hasattr(fitted, 'predict_proba'):\n",
    "            scores = fitted.predict_proba(X_te)[:, 1]\n",
    "        elif hasattr(fitted, 'decision_function'):\n",
    "            decision = fitted.decision_function(X_te)\n",
    "            scores = 1 / (1 + np.exp(-decision))\n",
    "        else:\n",
    "            scores = fitted.predict(X_te)\n",
    "    else:\n",
    "        scores = fitted.predict(X_te)\n",
    "    scores = np.clip(scores, 0, 1)\n",
    "    y_pred = (scores >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_te, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'threshold': threshold,\n",
    "        'fitted': fitted,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=15, weights='distance'))\n",
    "])\n",
    "\n",
    "lin_reg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', logreg, 'classifier'),\n",
    "    ('Random Forest Regressor', rf_reg, 'regressor'),\n",
    "    ('K-Nearest Neighbors', knn, 'classifier'),\n",
    "    ('Linear Regression', lin_reg, 'regressor'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43e76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (baseline): acc=0.713, prec=0.750, rec=0.684, f1=0.715\n",
      "Random Forest Regressor (main): acc=0.733, prec=0.783, rec=0.684, f1=0.730\n",
      "K-Nearest Neighbors: acc=0.700, prec=0.743, rec=0.658, f1=0.698\n",
      "Linear Regression: acc=0.707, prec=0.740, rec=0.684, f1=0.711\n",
      "Random Forest Regressor (main): acc=0.733, prec=0.783, rec=0.684, f1=0.730\n",
      "K-Nearest Neighbors: acc=0.700, prec=0.743, rec=0.658, f1=0.698\n",
      "Linear Regression: acc=0.707, prec=0.740, rec=0.684, f1=0.711\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>threshold</th>\n",
       "      <th>scores_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Regressor (main)</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.34704761904761905, 0.6384583333333333, 0.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (baseline)</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.715232</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.36086107043751325, 0.6487105053492047, 0.32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.39354325075311575, 0.613407923665401, 0.376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.697987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[0.3997930718654148, 0.6836890744102337, 0.541...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  precision    recall        f1  \\\n",
       "1  Random Forest Regressor (main)  0.733333   0.782609  0.683544  0.729730   \n",
       "0  Logistic Regression (baseline)  0.713333   0.750000  0.683544  0.715232   \n",
       "3               Linear Regression  0.706667   0.739726  0.683544  0.710526   \n",
       "2             K-Nearest Neighbors  0.700000   0.742857  0.658228  0.697987   \n",
       "\n",
       "   threshold                                     scores_preview  \n",
       "1        0.5  [0.34704761904761905, 0.6384583333333333, 0.52...  \n",
       "0        0.5  [0.36086107043751325, 0.6487105053492047, 0.32...  \n",
       "3        0.5  [0.39354325075311575, 0.613407923665401, 0.376...  \n",
       "2        0.5  [0.3997930718654148, 0.6836890744102337, 0.541...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "results = []\n",
    "fitted_models: Dict[str, Any] = {}\n",
    "\n",
    "for name, model, kind in models:\n",
    "    try:\n",
    "        metrics = evaluate_model(model, X_train, y_train, X_test, y_test, kind)\n",
    "        fitted_models[name] = metrics.pop('fitted')\n",
    "        results.append({'model': name, **metrics})\n",
    "        print(\n",
    "            f\"{name}: acc={metrics['accuracy']:.3f}, prec={metrics['precision']:.3f}, \"\n",
    "            f\"rec={metrics['recall']:.3f}, f1={metrics['f1']:.3f}\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f'{name} failed during training/evaluation: {exc}')\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by='f1', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a89767",
   "metadata": {},
   "source": [
    "## Results summary and justification\n",
    "\n",
    "**Random Forest Regressor (main model):**\n",
    "  - Accuracy: 0.733, Precision: 0.783, Recall: 0.684, F1: 0.730 (threshold=0.5).\n",
    "  - Interpretation: Highest F1 among candidates indicates best balance of precision/recall. Non-linear trees capture interactions between gold advantage, lane stats, and early objectives. This supports selecting Random Forest as the main model.\n",
    "\n",
    "**Logistic Regression (baseline model):**\n",
    "  - Accuracy: 0.713, Precision: 0.750, Recall: 0.684, F1: 0.715.\n",
    "  - Interpretation: Strong, interpretable baseline with competitive metrics. Coefficients (if examined) would likely align with key signals (e.g., gold diff, first tower/dragon). Good for sanity checks and fast iterations.\n",
    "\n",
    "**Linear Regression:**\n",
    "  - Accuracy: 0.707, Precision: 0.740, Recall: 0.684, F1: 0.711.\n",
    "  - Interpretation: As a regression converted to a classifier via thresholding, it performs close to Logistic Regression but slightly worse, consistent with its linear assumptions and lack of calibrated probabilities.\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "  - Accuracy: 0.700, Precision: 0.743, Recall: 0.658, F1: 0.698.\n",
    "  - Interpretation: Slightly lower F1. Likely impacted by feature scale sensitivity and the curse of dimensionality; despite scaling, KNN struggles with mixed, high-dimensional numeric feature spaces.\n",
    "\n",
    "**Summary:** Results match expectations - Random Forest leads and remains the recommended main model; Logistic Regression is a transparent baseline; KNN and Linear Regression are useful comparisons but secondary choices given observed performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73219e41",
   "metadata": {},
   "source": [
    "## References\n",
    "- [R1] Scikit-learn documentation – Logistic Regression, Random Forests, KNN, Linear Regression: https://scikit-learn.org/stable/user_guide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
