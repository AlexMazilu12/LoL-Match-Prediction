{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5e2519",
   "metadata": {},
   "source": [
    "## League of Legends - Data Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eac94",
   "metadata": {},
   "source": [
    "Purpose: This notebook focuses strictly on acquiring and persisting the RAW Riot match data (match detail + timeline). No cleaning, feature engineering, or train/val/test splitting happens here — those steps live in the DAIA and Modeling notebook.\n",
    "\n",
    "Scope in this notebook:\n",
    "- Authenticate with Riot API and fetch raw match JSONs to `data/raw/`.\n",
    "- Maintain a lightweight `matchlist.json` to track which matches are available locally.\n",
    "\n",
    "Out of scope (handled in DAIA & Modeling):\n",
    "- Building pregame or 15-minute feature tables.\n",
    "- Cleaning, filtering, or schema validation.\n",
    "- Train/validation/test splits and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f745",
   "metadata": {},
   "source": [
    "## Introduction & Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a03ba6",
   "metadata": {},
   "source": [
    "**Goals of this notebook:**\n",
    "\n",
    "-   Collect match data from the Riot API.\n",
    "\n",
    "-   Parse and extract both pre-game features (champion picks, roles) and early-game features at 15 minutes (gold diff, kills, objectives).\n",
    "\n",
    "-   Produce a cleaned, versioned dataset (CSV) with clear documentation for each column.\n",
    "\n",
    "**Why separate processing from modeling?**\n",
    "\n",
    "Riot match timelines and match lists can become large (MBs–GBs). Processing and cleaning in a dedicated notebook or script avoids repeated heavy work during model experimentation and keeps the ML notebook focused on training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a268b2",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI)\n",
    "\n",
    "### Key Concepts\n",
    "- XAI = Explainable Artificial Intelligence; methods and techniques to make AI models transparent and understandable to humans.\n",
    "- **Global explanations**: Show which features are most influential on average across a dataset.\n",
    "- **Local explanations**: Explain why a specific prediction was made.\n",
    "- **Intrinsic vs Post-hoc**: \n",
    "  - **Intrinsic**: Models that are transparent by design (e.g., decision trees, linear models).  \n",
    "  - **Post-hoc**: Methods applied to black-box models after training (e.g., SHAP, LIME, counterfactuals).\n",
    "- **Visualization & communication**: Plots, summary graphs, or textual explanations make insights accessible for different audiences.\n",
    "\n",
    "### Pros of XAI\n",
    "- **Transparency:** Users understand why a model makes a prediction.\n",
    "- **Trust:** Increases adoption by showing clear reasoning.\n",
    "- **Debugging & improvement:** Identifies if models rely on meaningful or misleading signals.\n",
    "- **Actionable insights:** Helps make decisions in strategy, operations, or coaching.\n",
    "- **Flexible communication:** Tailored explanations for technical and non-technical users.\n",
    "\n",
    "### Cons and Dangers\n",
    "- **Complexity:** Adds extra steps to modeling pipelines.\n",
    "- **Misinterpretation:** Oversimplified explanations may mislead users.\n",
    "- **Trade-off with accuracy:** Simple interpretable models may be less accurate; black-box models need complex post-hoc explanations.\n",
    "- **Computational cost:** Methods like SHAP can be resource-intensive on large datasets.\n",
    "- **Partial explanations:** Some methods only approximate reasoning, which can be risky if users over-trust them.\n",
    "- **Ethical & safety concerns:** Revealing certain features or internal logic may expose vulnerabilities (e.g., security risks, sensitive data leaks).\n",
    "\n",
    "## Application Across Domains\n",
    "\n",
    "#### Games (General)\n",
    "- XAI is used to explain AI behavior in a wide range of games, from strategy to simulation:  \n",
    "  - **NPC behavior**: Why an AI-controlled character made a certain move or decision.  \n",
    "  - **Dynamic difficulty adjustment (DDA)**: Why the game adapts difficulty based on player performance.  \n",
    "  - **Analytics and predictions**: Explaining why an AI predicts a certain outcome or strategy is likely to succeed.  \n",
    "- Benefits: Helps developers debug and balance games, improves player trust, and provides educational insight into gameplay mechanics.  \n",
    "- Challenges: High-dimensional state spaces, real-time decision-making, and latent human factors like coordination or communication are hard to capture.\n",
    "\n",
    "#### League of Legends (Specific)\n",
    "- Early-game gold, first tower, and objectives are strong predictors of match outcomes.  \n",
    "- XAI helps **coaches, players, and analysts** understand *why* a team is favored.  \n",
    "- Limitations: Some factors like team synergy, communication, or morale cannot be measured numerically.  \n",
    "- Goal: Make predictions interpretable and actionable in real time without compromising fairness or strategy secrecy.\n",
    "\n",
    "#### Science and Engineering\n",
    "- XAI explains results from simulations, experiments, or predictive maintenance models.\n",
    "- Enables **trust, safety, and accountability**: Users must understand why models make predictions that impact real-world systems.\n",
    "- Examples: AI flagging faulty machinery, medical diagnosis, or chemical simulations.\n",
    "- Challenge: Balancing **high performance** (accuracy) and **interpretability** is critical for safety.\n",
    "\n",
    "#### Ethical and Regulatory Considerations\n",
    "- XAI is increasingly required for ethical, legal, and safety compliance (e.g., GDPR in Europe, AI safety standards).\n",
    "- Certain aspects of models should **not be fully disclosed** to prevent misuse:\n",
    "  - Trade secrets or proprietary algorithms.\n",
    "  - Sensitive personal data in healthcare or finance.\n",
    "  - Safety-critical logic where exposing details could be dangerous (e.g., autonomous vehicles or critical infrastructure).\n",
    "- Regulation ensures transparency without compromising security or privacy.\n",
    "\n",
    "### What to Explain and What Not to Explain\n",
    "- **Explain:**  \n",
    "  - Key predictive features and their contributions.  \n",
    "  - Model reasoning that impacts human decision-making.  \n",
    "  - Patterns that help users learn, plan, or optimize strategy.\n",
    "- **Do not explain fully:**  \n",
    "  - Sensitive data or personally identifiable information.  \n",
    "  - Internal logic that could create security risks.  \n",
    "  - Proprietary algorithms where disclosure is not allowed.\n",
    "\n",
    "### Personal Learning Takeaways\n",
    "- XAI bridges the gap between AI predictions and human understanding, making models more trustworthy and actionable.\n",
    "- Choosing the right model depends on **accuracy, interpretability, and domain needs**.\n",
    "- Safety, ethics, and regulations are as important as performance: what we explain and what we hide must be carefully considered.\n",
    "- Applications differ by domain: in games, clarity and strategic insight are priorities; in science or engineering, safety, reliability, and reproducibility are critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1533",
   "metadata": {},
   "source": [
    "## Environment & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "RIOT_API_KEY = os.getenv('RIOT_API_KEY')\n",
    "if not RIOT_API_KEY:\n",
    "    raise RuntimeError('Set RIOT_API_KEY as an environment variable or in a .env file')\n",
    "\n",
    "\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103b96e",
   "metadata": {},
   "source": [
    "## Data sourcing (Riot API overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d871",
   "metadata": {},
   "source": [
    "### What endpoints we be used:\n",
    "\n",
    "- **Match IDs** by **PUUID:** /lol/match/v5/matches/by-puuid/{puuid}/ids - returns a list of match IDs for a given player.\n",
    "\n",
    "- **Match detail by matchId**: /lol/match/v5/matches/{matchId} — returns the full match info and timeline (timelines might be included alongside).\n",
    "\n",
    "### Filtering notes:\n",
    "\n",
    "- Use **queue=420** to restrict to **Ranked Solo/Duo matches (competitive).**\n",
    "\n",
    "- Filter out **remakes** or **extremely short games** (e.g., gameDuration < 9 minutes) to avoid noisy examples.\n",
    "\n",
    "### Rate limits & best practices:\n",
    "\n",
    "- Riot enforces **rate limits**, so I will be using **time.sleep** to pause between requests and wait longer if blocked (HTTP 429).\n",
    "\n",
    "- **Cache** raw match JSONs to disk so you don't re-download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf40fb2",
   "metadata": {},
   "source": [
    "## Data requirements and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd5221",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Core Features (and why)\n",
    "\n",
    "- **Match metadata**: `matchId`, `queueId`, `gameDuration` - needed to filter matches.\n",
    "- **Champion & role picks**: champion IDs/names, team positions - needed for pre-game features.\n",
    "- **Lane metrics at 15 minutes**: gold diff, CS diff, XP diff, K/D/A per lane - strong early indicators.\n",
    "- **Team objectives by 15 minutes**: `firstTower`, `firstDragon`, `firstHerald`, void grubs (`horde`).\n",
    "- **Target variables**: `blue_win` (0/1) and `win_probability` (for model output).\n",
    "\n",
    "\n",
    "\n",
    "### 2. Proposed Output Schema (one row per match)\n",
    "\n",
    "### Match Metadata\n",
    "- `matchId` (`str`)\n",
    "- `queueId` (`int`)\n",
    "- `gameDuration` (`int`, seconds)\n",
    "- `blue_win` (`0/1`)\n",
    "\n",
    "### Champion Picks & Roles\n",
    "- `blue_champions` (list of champion IDs)\n",
    "- `red_champions` (list of champion IDs)\n",
    "- `blue_roles` (list of positions)\n",
    "- `red_roles` (list of positions)\n",
    "\n",
    "### Early Features (15 min)\n",
    "- `blue_gold_15`, `red_gold_15`, `gold_diff_15` - difference in gold at 15 minutes\n",
    "- `blue_cs_15`, `red_cs_15`, `cs_diff_15` - difference in farm at 15 minutes\n",
    "- `blue_xp_15`, `red_xp_15`, `xp_diff_15` - difference in XP at 15 minutes\n",
    "- `blue_kills_15`, `red_kills_15`, `kills_diff_15` - difference in kills at 15 minutes\n",
    "\n",
    "### Objectives (15 min)\n",
    "- `first_tower` (None / blue / red)\n",
    "- `first_dragon` (None / blue / red)\n",
    "- `first_herald` (None / blue / red)\n",
    "- `first_grub` (None / blue / red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef368c3",
   "metadata": {},
   "source": [
    "## Storage strategy & versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e191",
   "metadata": {},
   "source": [
    "For this project, storage is intentionally simple in this notebook: we cache RAW JSON responses under `data/raw/` and maintain `matchlist.json`. Processed/cleaned tables are created downstream in the DAIA & Modeling notebook.\n",
    "\n",
    "- Raw data: every match JSON from the Riot API lives in `data/raw/` (both match detail and timeline). We never overwrite these so we can always reprocess.\n",
    "- Match index: `matchlist.json` lists the match IDs we have locally.\n",
    "\n",
    "Note: Processed artifacts (Parquet/CSV) will be written by the DAIA notebook, not here. This keeps acquisition (this notebook) separate from processing/modeling (DAIA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f19a3",
   "metadata": {},
   "source": [
    "## Raw data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458f521",
   "metadata": {},
   "source": [
    "### Important initial design choices:\n",
    "\n",
    "- I am going to collect match IDs by querying several PUUIDs (players). Prefer high-activity public accounts.\n",
    "\n",
    "- For initial proof-of-concept, I will collect 1,000–5,000 matches. For final model I will have tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30755ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUUIDs fetched: ['T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading matches for T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg: 100%|██████████| 20/20 [00:00<00:00, 1362.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded 20 matches to 'data/raw/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants + basic Riot fetch helpers (simple version)\n",
    "REGION = 'europe'  # riot regional routing\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}  # auth header\n",
    "MATCHLIST_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/by-puuid/{{puuid}}/ids'\n",
    "MATCH_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}'\n",
    "TIMELINE_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}/timeline'\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Basic GET with tiny retry + rate limit handling\n",
    "def safe_get(url, params=None, retries=5):\n",
    "    for i in range(retries):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "        elif resp.status_code == 429:  # rate limited\n",
    "            wait = int(resp.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to GET {url} after {retries} retries\")\n",
    "\n",
    "# Get player unique id from riot name + tag\n",
    "def get_puuid(game_name, tag_line):\n",
    "    url = f\"https://{REGION}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{game_name}/{tag_line}\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json().get(\"puuid\")\n",
    "    print(f\"Failed to get PUUID for {game_name}#{tag_line}: {resp.status_code}\")\n",
    "    return None\n",
    "\n",
    "# Just grab some match ids for one player (solo queue)\n",
    "def fetch_match_ids(puuid, count=20, queue=420):\n",
    "    params = {\"start\": 0, \"count\": count, \"queue\": queue}\n",
    "    return safe_get(MATCHLIST_URL.format(puuid=puuid), params=params)\n",
    "\n",
    "# Save both match + timeline JSON locally if missing (cache)\n",
    "def fetch_and_save_match_with_timeline(match_id):\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    match_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(match_path):  # don't redownload\n",
    "        match_data = safe_get(MATCH_URL.format(matchId=match_id))\n",
    "        with open(match_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(match_data, f)\n",
    "\n",
    "    timeline_path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(timeline_path):\n",
    "        timeline_data = safe_get(TIMELINE_URL.format(matchId=match_id))\n",
    "        with open(timeline_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(timeline_data, f)\n",
    "\n",
    "    return match_path, timeline_path\n",
    "\n",
    "# Generic fetch/save helper\n",
    "def fetch_and_save_json(url, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):  # already there\n",
    "        return path\n",
    "    data = safe_get(url)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    return path\n",
    "\n",
    "# Single test player (can add more later)\n",
    "players = [{\"name\": \"BB99\", \"tag\": \"BLZNT\"}]\n",
    "\n",
    "# Resolve their PUUIDs\n",
    "for p in players:\n",
    "    p[\"puuid\"] = get_puuid(p[\"name\"], p[\"tag\"])\n",
    "puuids = [p[\"puuid\"] for p in players if p.get(\"puuid\")]\n",
    "print(\"PUUIDs fetched:\", puuids)\n",
    "\n",
    "# Rate limiting guard (rough)\n",
    "all_match_ids = []\n",
    "MAX_REQUESTS = 99  # under 100 per 2 min window\n",
    "WINDOW = 120\n",
    "request_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for puuid in puuids:\n",
    "    match_ids = fetch_match_ids(puuid, count=20)  # small sample\n",
    "    all_match_ids.extend(match_ids)\n",
    "    \n",
    "    for match_id in tqdm(match_ids, desc=f\"Downloading matches for {puuid}\"):\n",
    "        # Full match\n",
    "        fetch_and_save_json(MATCH_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}.json\"))\n",
    "        # Timeline\n",
    "        fetch_and_save_json(TIMELINE_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}_timeline.json\"))\n",
    "        \n",
    "        request_count += 2  # two calls per match\n",
    "        if request_count >= MAX_REQUESTS:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0, WINDOW - elapsed)\n",
    "            if sleep_time > 0:\n",
    "                print(f\"Sleeping {sleep_time:.1f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "            start_time = time.time()\n",
    "            request_count = 0\n",
    "\n",
    "# Persist the collected list\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(all_match_ids, f, indent=2)\n",
    "\n",
    "print(f\" Downloaded {len(all_match_ids)} matches to '{RAW_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b9bfc",
   "metadata": {},
   "source": [
    "### Regenerate matchlist.json after adding new matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4431f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated matchlist.json with 10000 match IDs from data/raw\n"
     ]
    }
   ],
   "source": [
    "# Regenerate matchlist.json from all match JSON files in data/raw\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "match_ids = []\n",
    "for fname in os.listdir(raw_dir):\n",
    "    if fname.endswith('.json') and not fname.endswith('_timeline.json'):\n",
    "        match_id = fname.replace('.json', '')\n",
    "        match_ids.append(match_id)\n",
    "\n",
    "# Remove duplicates, just in case\n",
    "match_ids = sorted(set(match_ids))\n",
    "\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(match_ids, f, indent=2)\n",
    "\n",
    "print(f\"Updated matchlist.json with {len(match_ids)} match IDs from {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6456415",
   "metadata": {},
   "source": [
    "## Processing & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46cc98",
   "metadata": {},
   "source": [
    "### Design approach (how I actually set this up)\n",
    "\n",
    "\n",
    "\n",
    "To keep it simple the data is split into two parts based on when the state of the game:\n",
    "\n",
    "\n",
    "\n",
    "1. **Pregame table** - only info known before the game starts:\n",
    "\n",
    "   - Champion picks by lane (top, jg, mid, bot, support) for each team.\n",
    "\n",
    "   - Match duration (just for quick filtering later).\n",
    "\n",
    "\n",
    "\n",
    "2. **15-minute table** - snapshot of early game:\n",
    "\n",
    "   - First objective takers (`tower`, `dragon`, `herald`, void grubs = `horde`).\n",
    "\n",
    "   - Team gold / xp / cs / kills and their diffs at 15:00.\n",
    "\n",
    "   - Games shorter than 15 minutes are skipped (remakes / noise).\n",
    "\n",
    "\n",
    "\n",
    "Both tables have one row per `matchId`, so joining them later is trivial.\n",
    "\n",
    "\n",
    "\n",
    "Why bother splitting? Faster iteration. I can train a draft-only model without loading timeline data, and I can tweak early game logic without touching the pregame export.\n",
    "\n",
    "\n",
    "\n",
    "Storage:\n",
    "\n",
    "- Parquet main files: `lol_pregame_data.parquet` and `lol_15min_data.parquet`.\n",
    "\n",
    "- Small CSV previews (`*_preview.csv`) for a quick look at the dataset\n",
    "\n",
    "\n",
    "\n",
    "No manifest, no version bumps - if I change logic I just overwrite the files as this is a small scale project. If I later need history I can start versioning.\n",
    "\n",
    "\n",
    "\n",
    "Flow I run:\n",
    "\n",
    "1. Regenerate `matchlist.json` from raw if needed.\n",
    "\n",
    "2. Build pregame (no timeline needed).\n",
    "\n",
    "3. Build 15m table (needs timeline + duration filter).\n",
    "\n",
    "4. Save Parquet + preview CSVs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f26b3",
   "metadata": {},
   "source": [
    "## Pregame Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8624d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregame rows written: 10000\n",
      "Missing raw match JSONs: 0\n",
      "Files saved:\n",
      " - data/processed\\lol_pregame_data.parquet\n",
      " - data/processed\\lol_pregame_data_preview.csv\n"
     ]
    }
   ],
   "source": [
    "# Build Pregame Table from local raw JSONs (no API calls)\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = 'data/raw'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Utility: normalize Riot position fields to our 5 lanes\n",
    "POS_MAP = {\n",
    "    'TOP': 'top',\n",
    "    'JUNGLE': 'jg',\n",
    "    'MIDDLE': 'mid',\n",
    "    'MID': 'mid',\n",
    "    'BOTTOM': 'bot',\n",
    "    'BOT': 'bot',\n",
    "    'UTILITY': 'support',\n",
    "    'SUPPORT': 'support',\n",
    "}\n",
    "\n",
    "def normalize_role(p):\n",
    "    # Prefer teamPosition, then individualPosition, then lane/role combos\n",
    "    team_pos = (p.get('teamPosition') or '').upper()\n",
    "    indiv_pos = (p.get('individualPosition') or '').upper()\n",
    "    lane = (p.get('lane') or '').upper()\n",
    "    role = (p.get('role') or '').upper()\n",
    "\n",
    "    for cand in (team_pos, indiv_pos, lane):\n",
    "        if cand in POS_MAP:\n",
    "            return POS_MAP[cand]\n",
    "\n",
    "    # Duo bottom handling\n",
    "    if lane in ('BOTTOM', 'BOT'):\n",
    "        if 'SUPPORT' in role or role == 'UTILITY':\n",
    "            return 'support'\n",
    "        return 'bot'\n",
    "\n",
    "    return None  # unknown\n",
    "\n",
    "# Load full match list\n",
    "with open('matchlist.json', 'r', encoding='utf-8') as f:\n",
    "    match_ids = json.load(f)\n",
    "\n",
    "rows = []\n",
    "skipped_missing = []\n",
    "for match_id in match_ids:\n",
    "    match_path = os.path.join(RAW_DIR, f'{match_id}.json')\n",
    "    if not os.path.exists(match_path):\n",
    "        skipped_missing.append(match_id)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(match_path, 'r', encoding='utf-8') as f:\n",
    "            m = json.load(f)\n",
    "    except Exception:\n",
    "        skipped_missing.append(match_id)\n",
    "        continue\n",
    "\n",
    "    info = m.get('info', {})\n",
    "    parts = info.get('participants', []) or []\n",
    "    if len(parts) != 10:\n",
    "        skipped_missing.append(match_id)\n",
    "        continue\n",
    "\n",
    "    # Prepare containers for each team\n",
    "    blue = {'top': None, 'jg': None, 'mid': None, 'bot': None, 'support': None}\n",
    "    red = {'top': None, 'jg': None, 'mid': None, 'bot': None, 'support': None}\n",
    "\n",
    "    for p in parts:\n",
    "        champ = p.get('championName') or str(p.get('championId'))\n",
    "        team = 'blue' if p.get('teamId') == 100 else 'red'\n",
    "        r = normalize_role(p)\n",
    "        if r is None:\n",
    "            # If role unknown, try to infer jungle if smite used (optional, quick heuristic)\n",
    "            # We keep it simple: leave None; will remain blank if not inferable\n",
    "            pass\n",
    "        if team == 'blue':\n",
    "            if r in blue and blue[r] is None:\n",
    "                blue[r] = champ\n",
    "        else:\n",
    "            if r in red and red[r] is None:\n",
    "                red[r] = champ\n",
    "\n",
    "    row = {\n",
    "        'matchId': match_id,\n",
    "        'gameDuration': info.get('gameDuration'),\n",
    "        'bluetop': blue['top'],\n",
    "        'bluejg': blue['jg'],\n",
    "        'bluemid': blue['mid'],\n",
    "        'bluebot': blue['bot'],\n",
    "        'bluesupport': blue['support'],\n",
    "        'redtop': red['top'],\n",
    "        'redjg': red['jg'],\n",
    "        'redmid': red['mid'],\n",
    "        'redbot': red['bot'],\n",
    "        'redsupport': red['support'],\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "pregame_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save outputs (full + small preview)\n",
    "pregame_parquet = os.path.join(PROCESSED_DIR, 'lol_pregame_data.parquet')\n",
    "pregame_csv = os.path.join(PROCESSED_DIR, 'lol_pregame_data_preview.csv')\n",
    "\n",
    "pregame_df.to_parquet(pregame_parquet, index=False)\n",
    "pregame_df.head(50).to_csv(pregame_csv, index=False)\n",
    "\n",
    "print(f\"Pregame rows written: {len(pregame_df)}\")\n",
    "print(f\"Missing raw match JSONs: {len(skipped_missing)}\")\n",
    "print('Files saved:')\n",
    "print(' -', pregame_parquet)\n",
    "print(' -', pregame_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefdb45",
   "metadata": {},
   "source": [
    "## Fifteen Minute Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbe77b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_MINUTE set to 15 minutes\n",
      "Processed output directory: data/processed\n"
     ]
    }
   ],
   "source": [
    "# Setup for 15-minute processing\n",
    "# Define the target minute and output directory used below\n",
    "import os\n",
    "\n",
    "TARGET_MINUTE = 15  # minutes\n",
    "processed_dir = 'data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(f\"TARGET_MINUTE set to {TARGET_MINUTE} minutes\")\n",
    "print(f\"Processed output directory: {processed_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal helpers to load raw JSON and aggregate 15m participant stats\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "\n",
    "def load_match_json(match_id: str) -> Dict:\n",
    "    path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_timeline_json(match_id: str) -> Dict:\n",
    "    path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Aggregate participant stats at target minute\n",
    "def aggregate_participant_stats_at_minute(match_json: Dict, timeline_json: Dict, minute: int = None) -> Dict[int, Dict[str, int]]:\n",
    "    if minute is None:\n",
    "        minute = TARGET_MINUTE\n",
    "    target_ms = minute * 60_000\n",
    "\n",
    "    frames = timeline_json.get('info', {}).get('frames', [])\n",
    "    if not frames:\n",
    "        raise ValueError('Timeline has no frames')\n",
    "\n",
    "    # Find the latest frame at or before target time\n",
    "    frame_idx = None\n",
    "    for i, fr in enumerate(frames):\n",
    "        if fr.get('timestamp', 0) <= target_ms:\n",
    "            frame_idx = i\n",
    "        else:\n",
    "            break\n",
    "    if frame_idx is None:\n",
    "        raise ValueError(f'No frame available at or before {minute} minutes')\n",
    "\n",
    "    frame = frames[frame_idx]\n",
    "    part_frames = frame.get('participantFrames') or {}\n",
    "\n",
    "    # Initialize kills up to target frame from events\n",
    "    kills = {pid: 0 for pid in range(1, 11)}\n",
    "    for fr in frames[: frame_idx + 1]:\n",
    "        for ev in fr.get('events', []) or []:\n",
    "            if ev.get('type') == 'CHAMPION_KILL':\n",
    "                kid = ev.get('killerId')\n",
    "                if isinstance(kid, int) and 1 <= kid <= 10:\n",
    "                    kills[kid] = kills.get(kid, 0) + 1\n",
    "\n",
    "    out: Dict[int, Dict[str, int]] = {}\n",
    "    for k, pf in part_frames.items():\n",
    "        # Keys may be strings \"1\"..\"10\"\n",
    "        try:\n",
    "            pid = int(k)\n",
    "        except Exception:\n",
    "            continue\n",
    "        tg = pf.get('totalGold')\n",
    "        xp = pf.get('xp')\n",
    "        cs = pf.get('minionsKilled')\n",
    "        # Fallbacks\n",
    "        tg = 0 if tg is None else tg\n",
    "        xp = 0 if xp is None else xp\n",
    "        cs = 0 if cs is None else cs\n",
    "        out[pid] = {\n",
    "            'totalGold': int(tg),\n",
    "            'xp': int(xp),\n",
    "            'minionsKilled': int(cs),\n",
    "            'kills': int(kills.get(pid, 0)),\n",
    "        }\n",
    "\n",
    "    if not out:\n",
    "        raise ValueError('participantFrames missing at target minute')\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base match JSON files in data/raw: 10000\n",
      "Total match IDs from matchlist.json: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matches (15m):   9%|▊         | 860/10000 [00:40<07:11, 21.18it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     timeline_json = \u001b[43mload_timeline_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_id\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# need events for early stats\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     33\u001b[39m     missing_timeline.append(match_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mload_timeline_json\u001b[39m\u001b[34m(match_id)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:312\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Rebuild 15-minute table with correct loop placement\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You can set SAMPLE_N=None to process all matches; keep a small sample first for a quick sanity-check\n",
    "SAMPLE_N = None\n",
    "\n",
    "MIN_GAME_DURATION = TARGET_MINUTE * 60\n",
    "rows = []\n",
    "skipped_short, missing_timeline, skipped_snapshot = [], [], []\n",
    "\n",
    "ids_iter = match_ids_all\n",
    "if SAMPLE_N is not None and SAMPLE_N > 0:\n",
    "    ids_iter = match_ids_all[:SAMPLE_N]\n",
    "\n",
    "for match_id in tqdm(ids_iter, desc=f\"Processing matches (15m, SAMPLE_N={SAMPLE_N})\"):\n",
    "    try:\n",
    "        match_json = load_match_json(match_id)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "    game_duration = match_json.get('info', {}).get('gameDuration', 0)\n",
    "    if game_duration < MIN_GAME_DURATION:\n",
    "        skipped_short.append((match_id, game_duration))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        timeline_json = load_timeline_json(match_id)\n",
    "    except FileNotFoundError:\n",
    "        missing_timeline.append(match_id)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        stats_snapshot = aggregate_participant_stats_at_minute(match_json, timeline_json, minute=TARGET_MINUTE)\n",
    "    except ValueError as exc:\n",
    "        skipped_snapshot.append((match_id, str(exc)))\n",
    "        continue\n",
    "\n",
    "    teams = match_json['info']['teams']\n",
    "\n",
    "    def first_team(obj_key):\n",
    "        try:\n",
    "            if teams[0]['objectives'][obj_key]['first']:\n",
    "                return 'blue'\n",
    "            if teams[1]['objectives'][obj_key]['first']:\n",
    "                return 'red'\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    row = {\n",
    "        'matchId': match_id,\n",
    "        'queueId': match_json['info'].get('queueId'),\n",
    "        'gameDuration': game_duration,\n",
    "        'blue_win': int(teams[0]['win']),\n",
    "        'first_tower': first_team('tower'),\n",
    "        'first_dragon': first_team('dragon'),\n",
    "        'first_herald': first_team('riftHerald'),\n",
    "        'first_grub': first_team('horde'),\n",
    "    }\n",
    "\n",
    "    blue_ids = [p['participantId'] for p in match_json['info']['participants'][:5]]\n",
    "    red_ids = [p['participantId'] for p in match_json['info']['participants'][5:]]\n",
    "\n",
    "    row.update({\n",
    "        'blue_gold_15': sum(stats_snapshot.get(pid, {}).get('totalGold', 0) for pid in blue_ids),\n",
    "        'red_gold_15': sum(stats_snapshot.get(pid, {}).get('totalGold', 0) for pid in red_ids),\n",
    "        'blue_xp_15': sum(stats_snapshot.get(pid, {}).get('xp', 0) for pid in blue_ids),\n",
    "        'red_xp_15': sum(stats_snapshot.get(pid, {}).get('xp', 0) for pid in red_ids),\n",
    "        'blue_cs_15': sum(stats_snapshot.get(pid, {}).get('minionsKilled', 0) for pid in blue_ids),\n",
    "        'red_cs_15': sum(stats_snapshot.get(pid, {}).get('minionsKilled', 0) for pid in red_ids),\n",
    "        'blue_kills_15': sum(stats_snapshot.get(pid, {}).get('kills', 0) for pid in blue_ids),\n",
    "        'red_kills_15': sum(stats_snapshot.get(pid, {}).get('kills', 0) for pid in red_ids),\n",
    "    })\n",
    "\n",
    "    row['gold_diff_15'] = row['blue_gold_15'] - row['red_gold_15']\n",
    "    row['cs_diff_15'] = row['blue_cs_15'] - row['red_cs_15']\n",
    "    row['xp_diff_15'] = row['blue_xp_15'] - row['red_xp_15']\n",
    "    row['kills_diff_15'] = row['blue_kills_15'] - row['red_kills_15']\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# Build DataFrame and save\n",
    "fixed_df_15m = pd.DataFrame(rows)\n",
    "print(\n",
    "    f\"Processed: {len(ids_iter)} | Kept: {len(fixed_df_15m)} | \"\n",
    "    f\"Skipped<15m: {len(skipped_short)} | MissingTimeline: {len(missing_timeline)} | SnapshotErrors: {len(skipped_snapshot)}\"\n",
    ")\n",
    "\n",
    "# Only write if we have rows\n",
    "if len(fixed_df_15m) > 0:\n",
    "    fifteen_parquet = os.path.join(processed_dir, 'lol_15min_data.parquet')\n",
    "    fifteen_csv = os.path.join(processed_dir, 'lol_15min_data_preview.csv')\n",
    "    fixed_df_15m.to_parquet(fifteen_parquet, index=False)\n",
    "    fixed_df_15m.head(50).to_csv(fifteen_csv, index=False)\n",
    "    print('Saved 15-minute dataset files (Parquet + preview CSV) [corrected loop].')\n",
    "else:\n",
    "    print('No rows to write; check skip counters above.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
