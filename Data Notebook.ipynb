{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5e2519",
   "metadata": {},
   "source": [
    "## League of Legends - Data Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eac94",
   "metadata": {},
   "source": [
    "Purpose: This notebook documents a clear and reproducible Data Processing workflow for the League of Legends Match Prediction challange. It explains why each step is needed, the code to collect and transform Riot match data, and produces a clean dataset ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f745",
   "metadata": {},
   "source": [
    "## Introduction & Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd1b94",
   "metadata": {},
   "source": [
    "**Goals of this notebook:**\n",
    "\n",
    "-   Collect match data from the Riot API.\n",
    "\n",
    "-   Parse and extract both pre-game features (champion picks, roles) and early-game features at 10 minutes (gold diff, kills, objectives).\n",
    "\n",
    "-   Produce a cleaned, versioned dataset (CSV) with clear documentation for each column.\n",
    "\n",
    "**Why separate processing from modeling?**\n",
    "\n",
    "Riot match timelines and match lists can become large (MBs–GBs). Processing and cleaning in a dedicated notebook or script avoids repeated heavy work during model experimentation and keeps the ML notebook focused on training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1533",
   "metadata": {},
   "source": [
    "## Environment & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "RIOT_API_KEY = os.getenv('RIOT_API_KEY')\n",
    "if not RIOT_API_KEY:\n",
    "    raise RuntimeError('Set RIOT_API_KEY as an environment variable or in a .env file')\n",
    "\n",
    "\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103b96e",
   "metadata": {},
   "source": [
    "## Data sourcing (Riot API overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d871",
   "metadata": {},
   "source": [
    "### What endpoints we be used:\n",
    "\n",
    "- **Match IDs** by **PUUID:** /lol/match/v5/matches/by-puuid/{puuid}/ids - returns a list of match IDs for a given player.\n",
    "\n",
    "- **Match detail by matchId**: /lol/match/v5/matches/{matchId} — returns the full match info and timeline (timelines might be included alongside).\n",
    "\n",
    "### Filtering notes:\n",
    "\n",
    "- Use **queue=420** to restrict to **Ranked Solo/Duo matches (competitive).**\n",
    "\n",
    "- Filter out **remakes** or **extremely short games** (e.g., gameDuration < 9 minutes) to avoid noisy examples.\n",
    "\n",
    "### Rate limits & best practices:\n",
    "\n",
    "- Riot enforces **rate limits**, so I will be using **time.sleep** to pause between requests and wait longer if blocked (HTTP 429).\n",
    "\n",
    "- **Cache** raw match JSONs to disk so you don't re-download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf40fb2",
   "metadata": {},
   "source": [
    "## Data requirements and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd5221",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Core Features (and why)\n",
    "\n",
    "- **Match metadata**: `matchId`, `queueId`, `gameDuration` - needed to filter matches.\n",
    "- **Champion & role picks**: champion IDs/names, team positions - needed for pre-game features.\n",
    "- **Lane metrics at 10 minutes**: gold diff, CS diff, XP diff, K/D/A per lane - strong early indicators.\n",
    "- **Team objectives at 10 minutes**: `firstBlood`, `firstTower`, `firstDragon` (+ type), `firstHerald`, counts of towers/dragons.\n",
    "- **Target variables**: `blue_win` (0/1) and `win_probability` (for model output).\n",
    "\n",
    "\n",
    "\n",
    "### 2. Proposed Output Schema (one row per match)\n",
    "\n",
    "### Match Metadata\n",
    "- `matchId` (`str`)\n",
    "- `queueId` (`int`)\n",
    "- `gameDuration` (`int`, seconds)\n",
    "- `blue_win` (`0/1`)\n",
    "\n",
    "### Champion Picks & Roles\n",
    "- `blue_champions` (list of champion IDs)\n",
    "- `red_champions` (list of champion IDs)\n",
    "- `blue_roles` (list of positions)\n",
    "- `red_roles` (list of positions)\n",
    "\n",
    "### Early Features (10 min)\n",
    "- `blue_gold_10`, `red_gold_10`, `gold_diff_10` - difference in gold at 10 minutes\n",
    "- `blue_cs_10`, `red_cs_10`, `cs_diff_10` - difference in farm at 10 minutes\n",
    "- `blue_xp_10`, `red_xp_10`, `xp_diff_10` - difference in xp at 10 minutes\n",
    "- `blue_kills_10`, `red_kills_10`, `kills_diff_10`- difference in kills at 10 minutes\n",
    "\n",
    "### Objectives (10 min)\n",
    "- `first_blood` (None / blue / red)\n",
    "- `first_tower` (None / blue / red)\n",
    "- `first_dragon` (None / blue / red)\n",
    "- `first_rift_herald` (None / blue / red)\n",
    "- `blue_towers_10`, `red_towers_10` - tower kills at 10 minutes\n",
    "- `blue_dragons_10`, `red_dragons_10` - dragon kills at 10 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef368c3",
   "metadata": {},
   "source": [
    "## Storage strategy & versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e191",
   "metadata": {},
   "source": [
    "For this project, I decided to store the raw and processed match data in a way that’s easy to manage and scale. Here’s what I did:\n",
    "### Raw and Processed Data\n",
    "- **Raw data**: every match JSON I collect from the Riot API is saved under `data/raw/{matchId}json`. \n",
    "This way, I have a permanent copy of the original data that I can always reprocess if needed.\n",
    "- **Processed data**: I’m saving the cleaned dataset as  `data/processed/lol_matches_v001.parquet`.\n",
    "   I chose **Parquet** because:\n",
    "  - It’s **much smaller and faster** than CSV, which matters since I’ll eventually have thousands of matches.\n",
    "  - It keeps **data types** (int, float, string) correctly without me having to convert them manually.\n",
    "  - I can quickly load only the columns I need for analysis or modeling.\n",
    "- **Manifest**: I also create `data/processed/manifest.json` that tracks:\n",
    "  - the dataset version  \n",
    "  - number of rows  \n",
    "  - the date it was generated  \n",
    "  - the raw files used  \n",
    "\n",
    "  I could use CSV instead, which is easier to open in Excel or share, but for this size of data, Parquet makes working with it way faster and more efficient. I might save a CSV copy later if I want to show it.\n",
    "\n",
    "### Versioning\n",
    "\n",
    "- I use **semantic versioning** for the dataset: `v001`, `v002`, etc.  \n",
    "- The **raw JSON files are never overwritten**. If I need to reprocess or add more matches, I create a new processed version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f19a3",
   "metadata": {},
   "source": [
    "## Raw data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d4d23",
   "metadata": {},
   "source": [
    "### Important initial design choices:\n",
    "\n",
    "- I am going to collect match IDs by querying several PUUIDs (players). Prefer high-activity public accounts.\n",
    "\n",
    "- For initial proof-of-concept, I will collect 1,000–5,000 matches. For final model I will have tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30755ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUUIDs fetched: ['T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading matches for T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg: 100%|██████████| 20/20 [00:00<00:00, 3202.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded 20 matches to 'data/raw/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "REGION = 'europe'\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}\n",
    "MATCHLIST_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/by-puuid/{{puuid}}/ids'\n",
    "MATCH_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}'\n",
    "TIMELINE_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}/timeline'\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Functions!\n",
    "def safe_get(url, params=None, retries=5):\n",
    "    for i in range(retries):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "        elif resp.status_code == 429:  # rate limited\n",
    "            wait = int(resp.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to GET {url} after {retries} retries\")\n",
    "\n",
    "def get_puuid(game_name, tag_line):\n",
    "    url = f\"https://{REGION}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{game_name}/{tag_line}\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json().get(\"puuid\")\n",
    "    print(f\"Failed to get PUUID for {game_name}#{tag_line}: {resp.status_code}\")\n",
    "    return None\n",
    "\n",
    "def fetch_match_ids(puuid, count=20, queue=420):\n",
    "    params = {\"start\": 0, \"count\": count, \"queue\": queue}\n",
    "    return safe_get(MATCHLIST_URL.format(puuid=puuid), params=params)\n",
    "\n",
    "# Fetch and save match JSON and timeline JSON\n",
    "def fetch_and_save_match_with_timeline(match_id):\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    # Match JSON\n",
    "    match_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(match_path):\n",
    "        match_data = safe_get(MATCH_URL.format(matchId=match_id))\n",
    "        with open(match_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(match_data, f)\n",
    "\n",
    "    # Timeline JSON\n",
    "    timeline_path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(timeline_path):\n",
    "        timeline_data = safe_get(TIMELINE_URL.format(matchId=match_id))\n",
    "        with open(timeline_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(timeline_data, f)\n",
    "\n",
    "    return match_path, timeline_path\n",
    "\n",
    "# Fetch JSON from URL and save if it doesn't exist.\n",
    "def fetch_and_save_json(url, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    data = safe_get(url)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    return path\n",
    "\n",
    "# Players to fetch data from\n",
    "players = [{\"name\": \"BB99\", \"tag\": \"BLZNT\"}]\n",
    "\n",
    "# Fetch PUUIDs\n",
    "for p in players:\n",
    "    p[\"puuid\"] = get_puuid(p[\"name\"], p[\"tag\"])\n",
    "puuids = [p[\"puuid\"] for p in players if p.get(\"puuid\")]\n",
    "print(\"PUUIDs fetched:\", puuids)\n",
    "\n",
    "# Fetch matches with rate-limit handling\n",
    "all_match_ids = []\n",
    "MAX_REQUESTS = 99  # just under Riot's 100 requests per 2 min\n",
    "WINDOW = 120\n",
    "request_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for puuid in puuids:\n",
    "    match_ids = fetch_match_ids(puuid, count=20)\n",
    "    all_match_ids.extend(match_ids)\n",
    "    \n",
    "    for match_id in tqdm(match_ids, desc=f\"Downloading matches for {puuid}\"):\n",
    "        # Full match details\n",
    "        fetch_and_save_json(MATCH_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}.json\"))\n",
    "\n",
    "        # Match timeline\n",
    "        fetch_and_save_json(TIMELINE_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}_timeline.json\"))\n",
    "        \n",
    "        request_count += 2  # two requests per match\n",
    "        if request_count >= MAX_REQUESTS:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0, WINDOW - elapsed)\n",
    "            if sleep_time > 0:\n",
    "                print(f\"Sleeping {sleep_time:.1f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "            start_time = time.time()\n",
    "            request_count = 0\n",
    "\n",
    "# Save full list of match IDs\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(all_match_ids, f, indent=2)\n",
    "\n",
    "print(f\" Downloaded {len(all_match_ids)} matches to '{RAW_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b9bfc",
   "metadata": {},
   "source": [
    "### Regenerate matchlist.json after adding new matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4431f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated matchlist.json with 45 match IDs from data/raw\n"
     ]
    }
   ],
   "source": [
    "# Regenerate matchlist.json from all match JSON files in data/raw\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "match_ids = []\n",
    "for fname in os.listdir(raw_dir):\n",
    "    if fname.endswith('.json') and not fname.endswith('_timeline.json'):\n",
    "        match_id = fname.replace('.json', '')\n",
    "        match_ids.append(match_id)\n",
    "\n",
    "# Remove duplicates, just in case\n",
    "match_ids = sorted(set(match_ids))\n",
    "\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(match_ids, f, indent=2)\n",
    "\n",
    "print(f\"Updated matchlist.json with {len(match_ids)} match IDs from {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6456415",
   "metadata": {},
   "source": [
    "## Parsing & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46cc98",
   "metadata": {},
   "source": [
    "### Design approach (how I actually set this up)\n",
    "\n",
    "To keep it simple the data is split into two parts based on when the state of the game:\n",
    "\n",
    "1. **Pregame table** – only info known before the game starts:\n",
    "   - Match metadata (`id`, `queue`, `version`, `duration`).\n",
    "   - Champion picks (`blue` vs `red`) in pick order.\n",
    "   - Positions/roles (which champion is facing which and in what position).\n",
    "   Lists are stored as pipe-separated strings to keep it flat (easy to split back later).\n",
    "\n",
    "2. **10-minute table** – what the game state looks like at/around 10:00:\n",
    "   - First objective takers (`tower`, `dragon`, `herald`, grubs = `horde`).\n",
    "   - Team gold / xp / cs / kills and their diffs.\n",
    "   - I skip matches shorter than 10 min because those don’t have a real early game.\n",
    "\n",
    "Both tables have one row per `matchId`, so joining them later is trivial.\n",
    "\n",
    "Why bother splitting? Faster iteration. I can train a draft-only model without loading timeline data, and I can tweak early game logic without touching the pregame export.\n",
    "\n",
    "Storage: Parquet for the real thing, small CSV preview just so I can eyeball outputs. A `manifest.json` tracks versions + counts.\n",
    "\n",
    "Version bump any time I change schema or logic (e.g., add lane diffs → go to v002).\n",
    "\n",
    "Flow I run:\n",
    "1. Load raw JSON list.\n",
    "2. Build pregame (no timeline).\n",
    "3. Build 10m table (needs timeline + duration filter).\n",
    "4. Save Parquet/CSV + previews + update manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1f5b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of match JSON files in data/raw: 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of match JSON files in data/raw: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matches (10m): 100%|██████████| 45/45 [00:00<00:00, 123.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of match JSON files in data/raw: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matches (10m): 100%|██████████| 45/45 [00:00<00:00, 123.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in 10m DataFrame (>= 600s): 43\n",
      "Skipped 2 short games (<600s):\n",
      "  - EUN1_3831011312 (duration=110s)\n",
      "  - EUN1_3832486067 (duration=117s)\n",
      "           matchId  gameDuration first_tower first_dragon first_herald  \\\n",
      "0  EUN1_3830286977          2248         red         blue          red   \n",
      "1  EUN1_3830307017          1896         red         blue         blue   \n",
      "2  EUN1_3830671280           999        blue         blue         blue   \n",
      "3  EUN1_3830680693          2021         red          red         blue   \n",
      "4  EUN1_3830696669          1438        blue          red         blue   \n",
      "\n",
      "  first_grub  \n",
      "0        red  \n",
      "1       blue  \n",
      "2        red  \n",
      "3       blue  \n",
      "4       blue  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processed_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_10m[[\u001b[33m'\u001b[39m\u001b[33mmatchId\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mgameDuration\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfirst_tower\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfirst_dragon\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfirst_herald\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfirst_grub\u001b[39m\u001b[33m'\u001b[39m]].head())\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Save 10-minute table (Parquet + small CSV preview)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m tenmin_parquet = os.path.join(\u001b[43mprocessed_dir\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtenmin_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTENMIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     72\u001b[39m tenmin_csv = os.path.join(processed_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtenmin_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTENMIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_preview.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     73\u001b[39m df_10m.to_parquet(tenmin_parquet, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_dir' is not defined"
     ]
    }
   ],
   "source": [
    "#Process all matches\n",
    "raw_dir = 'data/raw'\n",
    "all_match_files = [f.replace('.json','') for f in os.listdir(raw_dir) if f.endswith('.json')]\n",
    "\n",
    "# Debug: print number of match JSON files in data/raw\n",
    "print(\"Number of match JSON files in data/raw:\", len([f for f in os.listdir(raw_dir) if f.endswith('.json')]))\n",
    "\n",
    "MIN_GAME_DURATION = 600  # seconds; skip remakes / ultra-short games\n",
    "rows = []\n",
    "skipped_short = []\n",
    "\n",
    "for match_id in tqdm(all_match_ids, desc=\"Processing matches\"):\n",
    "    match_json = load_match_json(match_id)\n",
    "    game_duration = match_json['info'].get('gameDuration', 0)\n",
    "    # Skip very short games (likely remakes)\n",
    "    if game_duration < MIN_GAME_DURATION:\n",
    "        skipped_short.append((match_id, game_duration))\n",
    "        continue\n",
    "    \n",
    "    timeline_json = load_timeline_json(match_id)\n",
    "    stats_10min = aggregate_participant_stats_at_10min(match_json, timeline_json)\n",
    "    \n",
    "    teams = match_json['info']['teams']\n",
    "    \n",
    "    def first_team(obj_key):\n",
    "        try:\n",
    "            if teams[0]['objectives'][obj_key]['first']:\n",
    "                return 'blue'\n",
    "            if teams[1]['objectives'][obj_key]['first']:\n",
    "                return 'red'\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "    \n",
    "    # Build dataset row\n",
    "    row = {\n",
    "        'matchId': match_id,\n",
    "        'queueId': match_json['info'].get('queueId'),\n",
    "        'gameDuration': game_duration,\n",
    "        'blue_win': int(match_json['info']['teams'][0]['win']),\n",
    "        # first objective takers (void grubs now keyed as 'horde' in current patches)\n",
    "        'first_tower': first_team('tower'),\n",
    "        'first_dragon': first_team('dragon'),\n",
    "        'first_herald': first_team('riftHerald'),\n",
    "        'first_grub': first_team('horde')\n",
    "    }\n",
    "    \n",
    "    # aggregate team-level stats\n",
    "    blue_ids = [p['participantId'] for p in match_json['info']['participants'][:5]]\n",
    "    red_ids = [p['participantId'] for p in match_json['info']['participants'][5:]]\n",
    "\n",
    "    row.update({\n",
    "        'blue_gold_10': sum(stats_10min[pid]['totalGold'] for pid in blue_ids),\n",
    "        'red_gold_10': sum(stats_10min[pid]['totalGold'] for pid in red_ids),\n",
    "        'blue_xp_10': sum(stats_10min[pid]['xp'] for pid in blue_ids),\n",
    "        'red_xp_10': sum(stats_10min[pid]['xp'] for pid in red_ids),\n",
    "        'blue_cs_10': sum(stats_10min[pid]['minionsKilled'] for pid in blue_ids),\n",
    "        'red_cs_10': sum(stats_10min[pid]['minionsKilled'] for pid in red_ids),\n",
    "        'blue_kills_10': sum(stats_10min[pid]['kills'] for pid in blue_ids),\n",
    "        'red_kills_10': sum(stats_10min[pid]['kills'] for pid in red_ids),\n",
    "    })\n",
    "    row['gold_diff_10'] = row['blue_gold_10'] - row['red_gold_10']\n",
    "    row['cs_diff_10'] = row['blue_cs_10'] - row['red_cs_10']\n",
    "    row['xp_diff_10'] = row['blue_xp_10'] - row['red_xp_10']\n",
    "    row['kills_diff_10'] = row['blue_kills_10'] - row['red_kills_10']\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Rows in final DataFrame (>= {MIN_GAME_DURATION}s):\", df.shape[0])\n",
    "if skipped_short:\n",
    "    print(f\"Skipped {len(skipped_short)} short games (<{MIN_GAME_DURATION}s):\")\n",
    "    for mid, dur in skipped_short:\n",
    "        print(f\"  - {mid} (duration={dur}s)\")\n",
    "print(df[['matchId','gameDuration','first_tower','first_dragon','first_herald','first_grub']].head())\n",
    "\n",
    "# Save processed dataset as CSV\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "df.to_csv('data/processed/lol_10min_matches.csv', index=False)\n",
    "print(\"✅ Saved processed dataset as CSV with first objective takers (using 'horde' for void grubs). Remakes filtered.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
