{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5e2519",
   "metadata": {},
   "source": [
    "## League of Legends - Data Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eac94",
   "metadata": {},
   "source": [
    "Purpose: This notebook documents a clear and reproducible Data Processing workflow for the League of Legends Match Prediction challange. It explains why each step is needed, the code to collect and transform Riot match data, and produces a clean dataset ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f745",
   "metadata": {},
   "source": [
    "## Introduction & Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd1b94",
   "metadata": {},
   "source": [
    "**Goals of this notebook:**\n",
    "\n",
    "-   Collect match data from the Riot API.\n",
    "\n",
    "-   Parse and extract both pre-game features (champion picks, roles) and early-game features at 15 minutes (gold diff, kills, objectives).\n",
    "\n",
    "-   Produce a cleaned, versioned dataset (CSV) with clear documentation for each column.\n",
    "\n",
    "**Why separate processing from modeling?**\n",
    "\n",
    "Riot match timelines and match lists can become large (MBs–GBs). Processing and cleaning in a dedicated notebook or script avoids repeated heavy work during model experimentation and keeps the ML notebook focused on training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1533",
   "metadata": {},
   "source": [
    "## Environment & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "RIOT_API_KEY = os.getenv('RIOT_API_KEY')\n",
    "if not RIOT_API_KEY:\n",
    "    raise RuntimeError('Set RIOT_API_KEY as an environment variable or in a .env file')\n",
    "\n",
    "\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103b96e",
   "metadata": {},
   "source": [
    "## Data sourcing (Riot API overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d871",
   "metadata": {},
   "source": [
    "### What endpoints we be used:\n",
    "\n",
    "- **Match IDs** by **PUUID:** /lol/match/v5/matches/by-puuid/{puuid}/ids - returns a list of match IDs for a given player.\n",
    "\n",
    "- **Match detail by matchId**: /lol/match/v5/matches/{matchId} — returns the full match info and timeline (timelines might be included alongside).\n",
    "\n",
    "### Filtering notes:\n",
    "\n",
    "- Use **queue=420** to restrict to **Ranked Solo/Duo matches (competitive).**\n",
    "\n",
    "- Filter out **remakes** or **extremely short games** (e.g., gameDuration < 9 minutes) to avoid noisy examples.\n",
    "\n",
    "### Rate limits & best practices:\n",
    "\n",
    "- Riot enforces **rate limits**, so I will be using **time.sleep** to pause between requests and wait longer if blocked (HTTP 429).\n",
    "\n",
    "- **Cache** raw match JSONs to disk so you don't re-download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf40fb2",
   "metadata": {},
   "source": [
    "## Data requirements and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd5221",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Core Features (and why)\n",
    "\n",
    "- **Match metadata**: `matchId`, `queueId`, `gameDuration` - needed to filter matches.\n",
    "- **Champion & role picks**: champion IDs/names, team positions - needed for pre-game features.\n",
    "- **Lane metrics at 15 minutes**: gold diff, CS diff, XP diff, K/D/A per lane - strong early indicators.\n",
    "- **Team objectives by 15 minutes**: `firstTower`, `firstDragon`, `firstHerald`, void grubs (`horde`).\n",
    "- **Target variables**: `blue_win` (0/1) and `win_probability` (for model output).\n",
    "\n",
    "\n",
    "\n",
    "### 2. Proposed Output Schema (one row per match)\n",
    "\n",
    "### Match Metadata\n",
    "- `matchId` (`str`)\n",
    "- `queueId` (`int`)\n",
    "- `gameDuration` (`int`, seconds)\n",
    "- `blue_win` (`0/1`)\n",
    "\n",
    "### Champion Picks & Roles\n",
    "- `blue_champions` (list of champion IDs)\n",
    "- `red_champions` (list of champion IDs)\n",
    "- `blue_roles` (list of positions)\n",
    "- `red_roles` (list of positions)\n",
    "\n",
    "### Early Features (15 min)\n",
    "- `blue_gold_15`, `red_gold_15`, `gold_diff_15` - difference in gold at 15 minutes\n",
    "- `blue_cs_15`, `red_cs_15`, `cs_diff_15` - difference in farm at 15 minutes\n",
    "- `blue_xp_15`, `red_xp_15`, `xp_diff_15` - difference in XP at 15 minutes\n",
    "- `blue_kills_15`, `red_kills_15`, `kills_diff_15` - difference in kills at 15 minutes\n",
    "\n",
    "### Objectives (15 min)\n",
    "- `first_tower` (None / blue / red)\n",
    "- `first_dragon` (None / blue / red)\n",
    "- `first_herald` (None / blue / red)\n",
    "- `first_grub` (None / blue / red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef368c3",
   "metadata": {},
   "source": [
    "## Storage strategy & versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e191",
   "metadata": {},
   "source": [
    "For this project, I decided to keep storage dead simple.\n",
    "### Raw and Processed Data\n",
    "- **Raw data**: every match JSON from the Riot API lives in `data/raw/` (both the match detail and its timeline). I never overwrite these so I can always reprocess.\n",
    "- **Processed data**: two Parquet files:\n",
    "  - `data/processed/lol_pregame_data.parquet` (lane champion picks)\n",
    "  - `data/processed/lol_15min_data.parquet` (early game stats + first objectives)\n",
    "- **Preview CSVs**: `lol_pregame_data_preview.csv` and `lol_15min_data_preview.csv` (just first 50 rows to eyeball).\n",
    "\n",
    "Why Parquet? Smaller + keeps types + faster to load. CSV is only for quick inspection.\n",
    "\n",
    "No manifest / no versioning right now — I just overwrite while iterating. If I later need reproducibility or experiment tracking, I can start saving versioned snapshots like `lol_pregame_data_v002.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f19a3",
   "metadata": {},
   "source": [
    "## Raw data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d4d23",
   "metadata": {},
   "source": [
    "### Important initial design choices:\n",
    "\n",
    "- I am going to collect match IDs by querying several PUUIDs (players). Prefer high-activity public accounts.\n",
    "\n",
    "- For initial proof-of-concept, I will collect 1,000–5,000 matches. For final model I will have tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30755ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 00:48:06,127 [INFO] Starting with 1000 existing matches\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfetch_eune_matches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m fetch_matches_main\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mfetch_matches_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--target-match-count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--matches-per-player\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--history-window\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:241\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(argv)\u001b[39m\n\u001b[32m    238\u001b[39m history_window = \u001b[38;5;28mmax\u001b[39m(batch_size, args.history_window)\n\u001b[32m    239\u001b[39m queue_filter = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args.queue_id < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m args.queue_id\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_gold_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pages_per_division\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpuuid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:122\u001b[39m, in \u001b[36miter_gold_entries\u001b[39m\u001b[34m(api, max_pages)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_pages + \u001b[32m1\u001b[39m):\n\u001b[32m    121\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEAGUE_ROUTE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lol/league/v4/entries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUEUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdivision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     entries: List[Dict] = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entries:\n\u001b[32m    124\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mNo more entries for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m page \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, division, page)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:113\u001b[39m, in \u001b[36mRiotAPI.get_json\u001b[39m\u001b[34m(self, url, params, retries)\u001b[39m\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGiving up on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}"
     ]
    }
   ],
   "source": [
    "# Constants + basic Riot fetch helpers (simple version)\n",
    "REGION = 'europe'  # riot regional routing\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}  # auth header\n",
    "MATCHLIST_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/by-puuid/{{puuid}}/ids'\n",
    "MATCH_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}'\n",
    "TIMELINE_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}/timeline'\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Basic GET with tiny retry + rate limit handling\n",
    "def safe_get(url, params=None, retries=5):\n",
    "    for i in range(retries):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "        elif resp.status_code == 429:  # rate limited\n",
    "            wait = int(resp.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to GET {url} after {retries} retries\")\n",
    "\n",
    "# Get player unique id from riot name + tag\n",
    "def get_puuid(game_name, tag_line):\n",
    "    url = f\"https://{REGION}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{game_name}/{tag_line}\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json().get(\"puuid\")\n",
    "    print(f\"Failed to get PUUID for {game_name}#{tag_line}: {resp.status_code}\")\n",
    "    return None\n",
    "\n",
    "# Just grab some match ids for one player (solo queue)\n",
    "def fetch_match_ids(puuid, count=20, queue=420):\n",
    "    params = {\"start\": 0, \"count\": count, \"queue\": queue}\n",
    "    return safe_get(MATCHLIST_URL.format(puuid=puuid), params=params)\n",
    "\n",
    "# Save both match + timeline JSON locally if missing (cache)\n",
    "def fetch_and_save_match_with_timeline(match_id):\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    match_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(match_path):  # don't redownload\n",
    "        match_data = safe_get(MATCH_URL.format(matchId=match_id))\n",
    "        with open(match_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(match_data, f)\n",
    "\n",
    "    timeline_path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(timeline_path):\n",
    "        timeline_data = safe_get(TIMELINE_URL.format(matchId=match_id))\n",
    "        with open(timeline_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(timeline_data, f)\n",
    "\n",
    "    return match_path, timeline_path\n",
    "\n",
    "# Generic fetch/save helper\n",
    "def fetch_and_save_json(url, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):  # already there\n",
    "        return path\n",
    "    data = safe_get(url)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    return path\n",
    "\n",
    "# Single test player (can add more later)\n",
    "players = [{\"name\": \"BB99\", \"tag\": \"BLZNT\"}]\n",
    "\n",
    "# Resolve their PUUIDs\n",
    "for p in players:\n",
    "    p[\"puuid\"] = get_puuid(p[\"name\"], p[\"tag\"])\n",
    "puuids = [p[\"puuid\"] for p in players if p.get(\"puuid\")]\n",
    "print(\"PUUIDs fetched:\", puuids)\n",
    "\n",
    "# Rate limiting guard (rough)\n",
    "all_match_ids = []\n",
    "MAX_REQUESTS = 99  # under 100 per 2 min window\n",
    "WINDOW = 120\n",
    "request_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for puuid in puuids:\n",
    "    match_ids = fetch_match_ids(puuid, count=20)  # small sample\n",
    "    all_match_ids.extend(match_ids)\n",
    "    \n",
    "    for match_id in tqdm(match_ids, desc=f\"Downloading matches for {puuid}\"):\n",
    "        # Full match\n",
    "        fetch_and_save_json(MATCH_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}.json\"))\n",
    "        # Timeline\n",
    "        fetch_and_save_json(TIMELINE_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}_timeline.json\"))\n",
    "        \n",
    "        request_count += 2  # two calls per match\n",
    "        if request_count >= MAX_REQUESTS:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0, WINDOW - elapsed)\n",
    "            if sleep_time > 0:\n",
    "                print(f\"Sleeping {sleep_time:.1f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "            start_time = time.time()\n",
    "            request_count = 0\n",
    "\n",
    "# Persist the collected list\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(all_match_ids, f, indent=2)\n",
    "\n",
    "print(f\" Downloaded {len(all_match_ids)} matches to '{RAW_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b9bfc",
   "metadata": {},
   "source": [
    "### Regenerate matchlist.json after adding new matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4431f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated matchlist.json with 45 match IDs from data/raw\n"
     ]
    }
   ],
   "source": [
    "# Regenerate matchlist.json from all match JSON files in data/raw\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "match_ids = []\n",
    "for fname in os.listdir(raw_dir):\n",
    "    if fname.endswith('.json') and not fname.endswith('_timeline.json'):\n",
    "        match_id = fname.replace('.json', '')\n",
    "        match_ids.append(match_id)\n",
    "\n",
    "# Remove duplicates, just in case\n",
    "match_ids = sorted(set(match_ids))\n",
    "\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(match_ids, f, indent=2)\n",
    "\n",
    "print(f\"Updated matchlist.json with {len(match_ids)} match IDs from {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6456415",
   "metadata": {},
   "source": [
    "## Processing & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46cc98",
   "metadata": {},
   "source": [
    "### Design approach (how I actually set this up)\n",
    "\n",
    "\n",
    "\n",
    "To keep it simple the data is split into two parts based on when the state of the game:\n",
    "\n",
    "\n",
    "\n",
    "1. **Pregame table** - only info known before the game starts:\n",
    "\n",
    "   - Champion picks by lane (top, jg, mid, bot, support) for each team.\n",
    "\n",
    "   - Match duration (just for quick filtering later).\n",
    "\n",
    "\n",
    "\n",
    "2. **15-minute table** - snapshot of early game:\n",
    "\n",
    "   - First objective takers (`tower`, `dragon`, `herald`, void grubs = `horde`).\n",
    "\n",
    "   - Team gold / xp / cs / kills and their diffs at 15:00.\n",
    "\n",
    "   - Games shorter than 15 minutes are skipped (remakes / noise).\n",
    "\n",
    "\n",
    "\n",
    "Both tables have one row per `matchId`, so joining them later is trivial.\n",
    "\n",
    "\n",
    "\n",
    "Why bother splitting? Faster iteration. I can train a draft-only model without loading timeline data, and I can tweak early game logic without touching the pregame export.\n",
    "\n",
    "\n",
    "\n",
    "Storage:\n",
    "\n",
    "- Parquet main files: `lol_pregame_data.parquet` and `lol_15min_data.parquet`.\n",
    "\n",
    "- Small CSV previews (`*_preview.csv`) for a quick look at the dataset\n",
    "\n",
    "\n",
    "\n",
    "No manifest, no version bumps - if I change logic I just overwrite the files as this is a small scale project. If I later need history I can start versioning.\n",
    "\n",
    "\n",
    "\n",
    "Flow I run:\n",
    "\n",
    "1. Regenerate `matchlist.json` from raw if needed.\n",
    "\n",
    "2. Build pregame (no timeline needed).\n",
    "\n",
    "3. Build 15m table (needs timeline + duration filter).\n",
    "\n",
    "4. Save Parquet + preview CSVs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7826503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.fetch_eune_matches import main as fetch_matches_main\n",
    "\n",
    "# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\n",
    "fetch_matches_main([\n",
    "    \"--target-match-count\", \"1000\",\n",
    "    \"--matches-per-player\", \"100\",\n",
    "    \"--history-window\", \"600\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f26b3",
   "metadata": {},
   "source": [
    "## Pregame Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pregame (all lanes): 100%|██████████| 1000/1000 [00:12<00:00, 79.41it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified pregame table saved: data/processed\\lol_pregame_data.parquet (rows=1000)\n",
      "Columns: ['matchId', 'gameDuration', 'bluetop', 'bluejg', 'bluemid', 'bluebot', 'bluesupport', 'redtop', 'redjg', 'redmid', 'redbot', 'redsupport']\n",
      "Empty lane value counts: {'bluetop': 0, 'bluejg': 0, 'bluemid': 0, 'bluebot': 0, 'bluesupport': 0, 'redtop': 0, 'redjg': 0, 'redmid': 0, 'redbot': 0, 'redsupport': 0}\n",
      "Sample non-empty rows:\n",
      "           matchId  gameDuration       bluetop    bluejg bluemid      bluebot  \\\n",
      "0  EUN1_3830286977          2248  FiddleSticks     Sylas    Ahri         Ashe   \n",
      "1  EUN1_3830307017          1896         Teemo     Viego   Akali  MissFortune   \n",
      "2  EUN1_3830671280           999      Volibear  Nocturne    Ashe      Caitlyn   \n",
      "3  EUN1_3830680693          2021        Illaoi     Shaco   Yasuo        Vayne   \n",
      "4  EUN1_3830696669          1438   Mordekaiser  MasterYi    Ekko        Vayne   \n",
      "\n",
      "  bluesupport        redtop    redjg    redmid   redbot redsupport  \n",
      "0        Bard  Heimerdinger   Khazix      Sett    Sivir   Malphite  \n",
      "1       Milio        Singed    Amumu  Katarina     Ashe       Zyra  \n",
      "2       Taric       DrMundo     Kayn      Hwei     Jhin     Velkoz  \n",
      "3       Brand      Vladimir  DrMundo    Syndra  Caitlyn      Milio  \n",
      "4        Pyke        Singed    Viego    Xerath     Jhin      Braum  \n"
     ]
    }
   ],
   "source": [
    "from scripts.fetch_eune_matches import main as fetch_matches_main\n",
    "\n",
    "# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\n",
    "fetch_matches_main([\n",
    "    \"--target-match-count\", \"1000\",\n",
    "    \"--matches-per-player\", \"100\",\n",
    "    \"--history-window\", \"600\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefdb45",
   "metadata": {},
   "source": [
    "## Fifteen Minute Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base match JSON files in data/raw: 1000\n",
      "Total match IDs from matchlist.json: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matches (15m): 100%|██████████| 1000/1000 [00:22<00:00, 45.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in 15m DataFrame (>= 900s): 998\n",
      "Skipped 2 short games (<900s)\n",
      "           matchId  gameDuration first_tower first_dragon first_herald  \\\n",
      "0  EUN1_3830286977          2248         red         blue          red   \n",
      "1  EUN1_3830307017          1896         red         blue         blue   \n",
      "2  EUN1_3830671280           999        blue         blue         blue   \n",
      "3  EUN1_3830680693          2021         red          red         blue   \n",
      "4  EUN1_3830696669          1438        blue          red         blue   \n",
      "\n",
      "  first_grub  \n",
      "0        red  \n",
      "1       blue  \n",
      "2        red  \n",
      "3       blue  \n",
      "4       blue  \n",
      "Saved 15-minute dataset files (Parquet + preview CSV).\n",
      "15m DataFrame columns: ['matchId', 'queueId', 'gameDuration', 'blue_win', 'first_tower', 'first_dragon', 'first_herald', 'first_grub', 'blue_gold_15', 'red_gold_15', 'blue_xp_15', 'red_xp_15', 'blue_cs_15', 'red_cs_15', 'blue_kills_15', 'red_kills_15', 'gold_diff_15', 'cs_diff_15', 'xp_diff_15', 'kills_diff_15']\n",
      "Objective value counts (first_tower): {'blue': 527, 'red': 471}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build 15-Minute Table\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "all_match_files = [f.replace('.json', '') for f in os.listdir(raw_dir) if f.endswith('.json') and not f.endswith('_timeline.json')]\n",
    "print(\"Number of base match JSON files in data/raw:\", len(all_match_files))\n",
    "\n",
    "\n",
    "# Load full list from matchlist.json\n",
    "with open('matchlist.json', 'r') as f:\n",
    "    match_ids_all = json.load(f)\n",
    "print('Total match IDs from matchlist.json:', len(match_ids_all))\n",
    "\n",
    "\n",
    "MIN_GAME_DURATION = TARGET_MINUTE * 60  # require full duration to compute 15m stats\n",
    "rows = []\n",
    "skipped_short = []\n",
    "missing_timeline = []\n",
    "skipped_snapshot = []\n",
    "\n",
    "# Process each match\n",
    "for match_id in tqdm(match_ids_all, desc=\"Processing matches (15m)\"):\n",
    "    try:\n",
    "        match_json = load_match_json(match_id)\n",
    "    except FileNotFoundError:\n",
    "        continue  # skip if match file missing\n",
    "    game_duration = match_json.get('info', {}).get('gameDuration', 0)\n",
    "    if game_duration < MIN_GAME_DURATION:  # ignore super short games (remakes)\n",
    "        skipped_short.append((match_id, game_duration))\n",
    "        continue\n",
    "    try:\n",
    "        timeline_json = load_timeline_json(match_id)  # need events for early stats\n",
    "    except FileNotFoundError:\n",
    "        missing_timeline.append(match_id)\n",
    "        continue\n",
    "    try:\n",
    "        stats_snapshot = aggregate_participant_stats_at_minute(match_json, timeline_json)\n",
    "    except ValueError as exc:\n",
    "        skipped_snapshot.append((match_id, str(exc)))\n",
    "        continue\n",
    "    teams = match_json['info']['teams']\n",
    "\n",
    "\n",
    "\n",
    "# Method to determine which team got first objective\n",
    "def first_team(obj_key):\n",
    "    try:\n",
    "        if teams[0]['objectives'][obj_key]['first']:\n",
    "            return 'blue'\n",
    "        if teams[1]['objectives'][obj_key]['first']:\n",
    "            return 'red'\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# base row\n",
    "row = {\n",
    "    'matchId': match_id,\n",
    "    'queueId': match_json['info'].get('queueId'),\n",
    "    'gameDuration': game_duration,\n",
    "    'blue_win': int(match_json['info']['teams'][0]['win']),  # 1 if blue won\n",
    "    'first_tower': first_team('tower'),\n",
    "    'first_dragon': first_team('dragon'),\n",
    "    'first_herald': first_team('riftHerald'),\n",
    "    'first_grub': first_team('horde'),  # void grubs are exposed as HORDE in the API\n",
    "}\n",
    "\n",
    "# participantId lists split by side\n",
    "blue_ids = [p['participantId'] for p in match_json['info']['participants'][:5]]\n",
    "red_ids = [p['participantId'] for p in match_json['info']['participants'][5:]]\n",
    "\n",
    "# aggregate team sums at 15m\n",
    "row.update({\n",
    "    'blue_gold_15': sum(stats_snapshot[pid]['totalGold'] for pid in blue_ids),\n",
    "    'red_gold_15': sum(stats_snapshot[pid]['totalGold'] for pid in red_ids),\n",
    "    'blue_xp_15': sum(stats_snapshot[pid]['xp'] for pid in blue_ids),\n",
    "    'red_xp_15': sum(stats_snapshot[pid]['xp'] for pid in red_ids),\n",
    "    'blue_cs_15': sum(stats_snapshot[pid]['minionsKilled'] for pid in blue_ids),\n",
    "    'red_cs_15': sum(stats_snapshot[pid]['minionsKilled'] for pid in red_ids),\n",
    "    'blue_kills_15': sum(stats_snapshot[pid]['kills'] for pid in blue_ids),\n",
    "    'red_kills_15': sum(stats_snapshot[pid]['kills'] for pid in red_ids),\n",
    "})\n",
    "\n",
    "# diffs (blue - red)\n",
    "row['gold_diff_15'] = row['blue_gold_15'] - row['red_gold_15']\n",
    "row['cs_diff_15'] = row['blue_cs_15'] - row['red_cs_15']\n",
    "row['xp_diff_15'] = row['blue_xp_15'] - row['red_xp_15']\n",
    "row['kills_diff_15'] = row['blue_kills_15'] - row['red_kills_15']\n",
    "rows.append(row)\n",
    "\n",
    "# Build DataFrame\n",
    "df_15m = pd.DataFrame(rows)\n",
    "\n",
    "# Save outputs (full + small preview)\n",
    "fifteen_parquet = os.path.join(processed_dir, 'lol_15min_data.parquet')\n",
    "fifteen_csv = os.path.join(processed_dir, 'lol_15min_data_preview.csv')\n",
    "df_15m.to_parquet(fifteen_parquet, index=False)\n",
    "df_15m.head(50).to_csv(fifteen_csv, index=False)\n",
    "print('Saved 15-minute dataset files (Parquet + preview CSV).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
