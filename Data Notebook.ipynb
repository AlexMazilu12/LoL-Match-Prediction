{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5e2519",
   "metadata": {},
   "source": [
    "## League of Legends - Data Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eac94",
   "metadata": {},
   "source": [
    "Purpose: This notebook focuses strictly on acquiring and persisting the RAW Riot match data (match detail + timeline). No cleaning, feature engineering, or train/val/test splitting happens here — those steps live in the DAIA and Modeling notebook.\n",
    "\n",
    "Scope in this notebook:\n",
    "- Authenticate with Riot API and fetch raw match JSONs to `data/raw/`.\n",
    "- Maintain a lightweight `matchlist.json` to track which matches are available locally.\n",
    "\n",
    "Out of scope (handled in DAIA & Modeling):\n",
    "- Building pregame or 15-minute feature tables.\n",
    "- Cleaning, filtering, or schema validation.\n",
    "- Train/validation/test splits and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f745",
   "metadata": {},
   "source": [
    "## Introduction & Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a03ba6",
   "metadata": {},
   "source": [
    "**Goals of this notebook:**\n",
    "\n",
    "-   Collect match data from the Riot API.\n",
    "\n",
    "-   Parse and extract both pre-game features (champion picks, roles) and early-game features at 15 minutes (gold diff, kills, objectives).\n",
    "\n",
    "-   Produce a cleaned, versioned dataset (CSV) with clear documentation for each column.\n",
    "\n",
    "**Why separate processing from modeling?**\n",
    "\n",
    "Riot match timelines and match lists can become large (MBs–GBs). Processing and cleaning in a dedicated notebook or script avoids repeated heavy work during model experimentation and keeps the ML notebook focused on training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a268b2",
   "metadata": {},
   "source": [
    "# Explainable AI (XAI)\n",
    "\n",
    "### Key Concepts\n",
    "- XAI = Explainable Artificial Intelligence; methods and techniques to make AI models transparent and understandable to humans.\n",
    "- **Global explanations**: Show which features are most influential on average across a dataset.\n",
    "- **Local explanations**: Explain why a specific prediction was made.\n",
    "- **Intrinsic vs Post-hoc**: \n",
    "  - **Intrinsic**: Models that are transparent by design (e.g., decision trees, linear models).  \n",
    "  - **Post-hoc**: Methods applied to black-box models after training (e.g., SHAP, LIME, counterfactuals).\n",
    "- **Visualization & communication**: Plots, summary graphs, or textual explanations make insights accessible for different audiences.\n",
    "\n",
    "### Pros of XAI\n",
    "- **Transparency:** Users understand why a model makes a prediction.\n",
    "- **Trust:** Increases adoption by showing clear reasoning.\n",
    "- **Debugging & improvement:** Identifies if models rely on meaningful or misleading signals.\n",
    "- **Actionable insights:** Helps make decisions in strategy, operations, or coaching.\n",
    "- **Flexible communication:** Tailored explanations for technical and non-technical users.\n",
    "\n",
    "### Cons and Dangers\n",
    "- **Complexity:** Adds extra steps to modeling pipelines.\n",
    "- **Misinterpretation:** Oversimplified explanations may mislead users.\n",
    "- **Trade-off with accuracy:** Simple interpretable models may be less accurate; black-box models need complex post-hoc explanations.\n",
    "- **Computational cost:** Methods like SHAP can be resource-intensive on large datasets.\n",
    "- **Partial explanations:** Some methods only approximate reasoning, which can be risky if users over-trust them.\n",
    "- **Ethical & safety concerns:** Revealing certain features or internal logic may expose vulnerabilities (e.g., security risks, sensitive data leaks).\n",
    "\n",
    "## Application Across Domains\n",
    "\n",
    "#### Games (General)\n",
    "- XAI is used to explain AI behavior in a wide range of games, from strategy to simulation:  \n",
    "  - **NPC behavior**: Why an AI-controlled character made a certain move or decision.  \n",
    "  - **Dynamic difficulty adjustment (DDA)**: Why the game adapts difficulty based on player performance.  \n",
    "  - **Analytics and predictions**: Explaining why an AI predicts a certain outcome or strategy is likely to succeed.  \n",
    "- Benefits: Helps developers debug and balance games, improves player trust, and provides educational insight into gameplay mechanics.  \n",
    "- Challenges: High-dimensional state spaces, real-time decision-making, and latent human factors like coordination or communication are hard to capture.\n",
    "\n",
    "#### League of Legends (Specific)\n",
    "- Early-game gold, first tower, and objectives are strong predictors of match outcomes.  \n",
    "- XAI helps **coaches, players, and analysts** understand *why* a team is favored.  \n",
    "- Limitations: Some factors like team synergy, communication, or morale cannot be measured numerically.  \n",
    "- Goal: Make predictions interpretable and actionable in real time without compromising fairness or strategy secrecy.\n",
    "\n",
    "#### Science and Engineering\n",
    "- XAI explains results from simulations, experiments, or predictive maintenance models.\n",
    "- Enables **trust, safety, and accountability**: Users must understand why models make predictions that impact real-world systems.\n",
    "- Examples: AI flagging faulty machinery, medical diagnosis, or chemical simulations.\n",
    "- Challenge: Balancing **high performance** (accuracy) and **interpretability** is critical for safety.\n",
    "\n",
    "#### Ethical and Regulatory Considerations\n",
    "- XAI is increasingly required for ethical, legal, and safety compliance (e.g., GDPR in Europe, AI safety standards).\n",
    "- Certain aspects of models should **not be fully disclosed** to prevent misuse:\n",
    "  - Trade secrets or proprietary algorithms.\n",
    "  - Sensitive personal data in healthcare or finance.\n",
    "  - Safety-critical logic where exposing details could be dangerous (e.g., autonomous vehicles or critical infrastructure).\n",
    "- Regulation ensures transparency without compromising security or privacy.\n",
    "\n",
    "### What to Explain and What Not to Explain\n",
    "- **Explain:**  \n",
    "  - Key predictive features and their contributions.  \n",
    "  - Model reasoning that impacts human decision-making.  \n",
    "  - Patterns that help users learn, plan, or optimize strategy.\n",
    "- **Do not explain fully:**  \n",
    "  - Sensitive data or personally identifiable information.  \n",
    "  - Internal logic that could create security risks.  \n",
    "  - Proprietary algorithms where disclosure is not allowed.\n",
    "\n",
    "### Personal Learning Takeaways\n",
    "- XAI bridges the gap between AI predictions and human understanding, making models more trustworthy and actionable.\n",
    "- Choosing the right model depends on **accuracy, interpretability, and domain needs**.\n",
    "- Safety, ethics, and regulations are as important as performance: what we explain and what we hide must be carefully considered.\n",
    "- Applications differ by domain: in games, clarity and strategic insight are priorities; in science or engineering, safety, reliability, and reproducibility are critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1533",
   "metadata": {},
   "source": [
    "## Environment & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "RIOT_API_KEY = os.getenv('RIOT_API_KEY')\n",
    "if not RIOT_API_KEY:\n",
    "    raise RuntimeError('Set RIOT_API_KEY as an environment variable or in a .env file')\n",
    "\n",
    "\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103b96e",
   "metadata": {},
   "source": [
    "## Data sourcing (Riot API overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d871",
   "metadata": {},
   "source": [
    "### What endpoints we be used:\n",
    "\n",
    "- **Match IDs** by **PUUID:** /lol/match/v5/matches/by-puuid/{puuid}/ids - returns a list of match IDs for a given player.\n",
    "\n",
    "- **Match detail by matchId**: /lol/match/v5/matches/{matchId} — returns the full match info and timeline (timelines might be included alongside).\n",
    "\n",
    "### Filtering notes:\n",
    "\n",
    "- Use **queue=420** to restrict to **Ranked Solo/Duo matches (competitive).**\n",
    "\n",
    "- Filter out **remakes** or **extremely short games** (e.g., gameDuration < 9 minutes) to avoid noisy examples.\n",
    "\n",
    "### Rate limits & best practices:\n",
    "\n",
    "- Riot enforces **rate limits**, so I will be using **time.sleep** to pause between requests and wait longer if blocked (HTTP 429).\n",
    "\n",
    "- **Cache** raw match JSONs to disk so you don't re-download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf40fb2",
   "metadata": {},
   "source": [
    "## Data requirements and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd5221",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Core Features (and why)\n",
    "\n",
    "- **Match metadata**: `matchId`, `queueId`, `gameDuration` - needed to filter matches.\n",
    "- **Champion & role picks**: champion IDs/names, team positions - needed for pre-game features.\n",
    "- **Lane metrics at 15 minutes**: gold diff, CS diff, XP diff, K/D/A per lane - strong early indicators.\n",
    "- **Team objectives by 15 minutes**: `firstTower`, `firstDragon`, `firstHerald`, void grubs (`horde`).\n",
    "- **Target variables**: `blue_win` (0/1) and `win_probability` (for model output).\n",
    "\n",
    "\n",
    "\n",
    "### 2. Proposed Output Schema (one row per match)\n",
    "\n",
    "### Match Metadata\n",
    "- `matchId` (`str`)\n",
    "- `queueId` (`int`)\n",
    "- `gameDuration` (`int`, seconds)\n",
    "- `blue_win` (`0/1`)\n",
    "\n",
    "### Champion Picks & Roles\n",
    "- `blue_champions` (list of champion IDs)\n",
    "- `red_champions` (list of champion IDs)\n",
    "- `blue_roles` (list of positions)\n",
    "- `red_roles` (list of positions)\n",
    "\n",
    "### Early Features (15 min)\n",
    "- `blue_gold_15`, `red_gold_15`, `gold_diff_15` - difference in gold at 15 minutes\n",
    "- `blue_cs_15`, `red_cs_15`, `cs_diff_15` - difference in farm at 15 minutes\n",
    "- `blue_xp_15`, `red_xp_15`, `xp_diff_15` - difference in XP at 15 minutes\n",
    "- `blue_kills_15`, `red_kills_15`, `kills_diff_15` - difference in kills at 15 minutes\n",
    "\n",
    "### Objectives (15 min)\n",
    "- `first_tower` (None / blue / red)\n",
    "- `first_dragon` (None / blue / red)\n",
    "- `first_herald` (None / blue / red)\n",
    "- `first_grub` (None / blue / red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef368c3",
   "metadata": {},
   "source": [
    "## Storage strategy & versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e191",
   "metadata": {},
   "source": [
    "For this project, storage is intentionally simple in this notebook: we cache RAW JSON responses under `data/raw/` and maintain `matchlist.json`. Processed/cleaned tables are created downstream in the DAIA & Modeling notebook.\n",
    "\n",
    "- Raw data: every match JSON from the Riot API lives in `data/raw/` (both match detail and timeline). We never overwrite these so we can always reprocess.\n",
    "- Match index: `matchlist.json` lists the match IDs we have locally.\n",
    "\n",
    "Note: Processed artifacts (Parquet/CSV) will be written by the DAIA notebook, not here. This keeps acquisition (this notebook) separate from processing/modeling (DAIA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f19a3",
   "metadata": {},
   "source": [
    "## Raw data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458f521",
   "metadata": {},
   "source": [
    "### Important initial design choices:\n",
    "\n",
    "- I am going to collect match IDs by querying several PUUIDs (players). Prefer high-activity public accounts.\n",
    "\n",
    "- For initial proof-of-concept, I will collect 1,000–5,000 matches. For final model I will have tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30755ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get PUUID for BB99#BLZNT: 401\n",
      "PUUIDs fetched: []\n",
      " Downloaded 0 matches to 'data/raw/'\n"
     ]
    }
   ],
   "source": [
    "# Constants + basic Riot fetch helpers (simple version)\n",
    "REGION = 'europe'  # riot regional routing\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}  # auth header\n",
    "MATCHLIST_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/by-puuid/{{puuid}}/ids'\n",
    "MATCH_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}'\n",
    "TIMELINE_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}/timeline'\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Basic GET with tiny retry + rate limit handling\n",
    "def safe_get(url, params=None, retries=5):\n",
    "    for i in range(retries):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "        elif resp.status_code == 429:  # rate limited\n",
    "            wait = int(resp.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to GET {url} after {retries} retries\")\n",
    "\n",
    "# Get player unique id from riot name + tag\n",
    "def get_puuid(game_name, tag_line):\n",
    "    url = f\"https://{REGION}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{game_name}/{tag_line}\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json().get(\"puuid\")\n",
    "    print(f\"Failed to get PUUID for {game_name}#{tag_line}: {resp.status_code}\")\n",
    "    return None\n",
    "\n",
    "# Just grab some match ids for one player (solo queue)\n",
    "def fetch_match_ids(puuid, count=20, queue=420):\n",
    "    params = {\"start\": 0, \"count\": count, \"queue\": queue}\n",
    "    return safe_get(MATCHLIST_URL.format(puuid=puuid), params=params)\n",
    "\n",
    "# Save both match + timeline JSON locally if missing (cache)\n",
    "def fetch_and_save_match_with_timeline(match_id):\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    match_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(match_path):  # don't redownload\n",
    "        match_data = safe_get(MATCH_URL.format(matchId=match_id))\n",
    "        with open(match_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(match_data, f)\n",
    "\n",
    "    timeline_path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(timeline_path):\n",
    "        timeline_data = safe_get(TIMELINE_URL.format(matchId=match_id))\n",
    "        with open(timeline_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(timeline_data, f)\n",
    "\n",
    "    return match_path, timeline_path\n",
    "\n",
    "# Generic fetch/save helper\n",
    "def fetch_and_save_json(url, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):  # already there\n",
    "        return path\n",
    "    data = safe_get(url)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    return path\n",
    "\n",
    "# Single test player (can add more later)\n",
    "players = [{\"name\": \"BB99\", \"tag\": \"BLZNT\"}]\n",
    "\n",
    "# Resolve their PUUIDs\n",
    "for p in players:\n",
    "    p[\"puuid\"] = get_puuid(p[\"name\"], p[\"tag\"])\n",
    "puuids = [p[\"puuid\"] for p in players if p.get(\"puuid\")]\n",
    "print(\"PUUIDs fetched:\", puuids)\n",
    "\n",
    "# Rate limiting guard (rough)\n",
    "all_match_ids = []\n",
    "MAX_REQUESTS = 99  # under 100 per 2 min window\n",
    "WINDOW = 120\n",
    "request_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for puuid in puuids:\n",
    "    match_ids = fetch_match_ids(puuid, count=20)  # small sample\n",
    "    all_match_ids.extend(match_ids)\n",
    "    \n",
    "    for match_id in tqdm(match_ids, desc=f\"Downloading matches for {puuid}\"):\n",
    "        # Full match\n",
    "        fetch_and_save_json(MATCH_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}.json\"))\n",
    "        # Timeline\n",
    "        fetch_and_save_json(TIMELINE_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}_timeline.json\"))\n",
    "        \n",
    "        request_count += 2  # two calls per match\n",
    "        if request_count >= MAX_REQUESTS:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0, WINDOW - elapsed)\n",
    "            if sleep_time > 0:\n",
    "                print(f\"Sleeping {sleep_time:.1f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "            start_time = time.time()\n",
    "            request_count = 0\n",
    "\n",
    "# Persist the collected list\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(all_match_ids, f, indent=2)\n",
    "\n",
    "print(f\" Downloaded {len(all_match_ids)} matches to '{RAW_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b9bfc",
   "metadata": {},
   "source": [
    "### Regenerate matchlist.json after adding new matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4431f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated matchlist.json with 1000 match IDs from data/raw\n"
     ]
    }
   ],
   "source": [
    "# Regenerate matchlist.json from all match JSON files in data/raw\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "match_ids = []\n",
    "for fname in os.listdir(raw_dir):\n",
    "    if fname.endswith('.json') and not fname.endswith('_timeline.json'):\n",
    "        match_id = fname.replace('.json', '')\n",
    "        match_ids.append(match_id)\n",
    "\n",
    "# Remove duplicates, just in case\n",
    "match_ids = sorted(set(match_ids))\n",
    "\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(match_ids, f, indent=2)\n",
    "\n",
    "print(f\"Updated matchlist.json with {len(match_ids)} match IDs from {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6456415",
   "metadata": {},
   "source": [
    "## Processing & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46cc98",
   "metadata": {},
   "source": [
    "### Design approach (how I actually set this up)\n",
    "\n",
    "\n",
    "\n",
    "To keep it simple the data is split into two parts based on when the state of the game:\n",
    "\n",
    "\n",
    "\n",
    "1. **Pregame table** - only info known before the game starts:\n",
    "\n",
    "   - Champion picks by lane (top, jg, mid, bot, support) for each team.\n",
    "\n",
    "   - Match duration (just for quick filtering later).\n",
    "\n",
    "\n",
    "\n",
    "2. **15-minute table** - snapshot of early game:\n",
    "\n",
    "   - First objective takers (`tower`, `dragon`, `herald`, void grubs = `horde`).\n",
    "\n",
    "   - Team gold / xp / cs / kills and their diffs at 15:00.\n",
    "\n",
    "   - Games shorter than 15 minutes are skipped (remakes / noise).\n",
    "\n",
    "\n",
    "\n",
    "Both tables have one row per `matchId`, so joining them later is trivial.\n",
    "\n",
    "\n",
    "\n",
    "Why bother splitting? Faster iteration. I can train a draft-only model without loading timeline data, and I can tweak early game logic without touching the pregame export.\n",
    "\n",
    "\n",
    "\n",
    "Storage:\n",
    "\n",
    "- Parquet main files: `lol_pregame_data.parquet` and `lol_15min_data.parquet`.\n",
    "\n",
    "- Small CSV previews (`*_preview.csv`) for a quick look at the dataset\n",
    "\n",
    "\n",
    "\n",
    "No manifest, no version bumps - if I change logic I just overwrite the files as this is a small scale project. If I later need history I can start versioning.\n",
    "\n",
    "\n",
    "\n",
    "Flow I run:\n",
    "\n",
    "1. Regenerate `matchlist.json` from raw if needed.\n",
    "\n",
    "2. Build pregame (no timeline needed).\n",
    "\n",
    "3. Build 15m table (needs timeline + duration filter).\n",
    "\n",
    "4. Save Parquet + preview CSVs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7826503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 11:20:34,249 [INFO] Starting with 1000 existing matches\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfetch_eune_matches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m fetch_matches_main\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mfetch_matches_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--target-match-count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--matches-per-player\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--history-window\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:241\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(argv)\u001b[39m\n\u001b[32m    238\u001b[39m history_window = \u001b[38;5;28mmax\u001b[39m(batch_size, args.history_window)\n\u001b[32m    239\u001b[39m queue_filter = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args.queue_id < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m args.queue_id\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_gold_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pages_per_division\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpuuid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:122\u001b[39m, in \u001b[36miter_gold_entries\u001b[39m\u001b[34m(api, max_pages)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_pages + \u001b[32m1\u001b[39m):\n\u001b[32m    121\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEAGUE_ROUTE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lol/league/v4/entries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUEUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdivision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     entries: List[Dict] = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entries:\n\u001b[32m    124\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mNo more entries for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m page \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, division, page)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:113\u001b[39m, in \u001b[36mRiotAPI.get_json\u001b[39m\u001b[34m(self, url, params, retries)\u001b[39m\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGiving up on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}"
     ]
    }
   ],
   "source": [
    "from scripts.fetch_eune_matches import main as fetch_matches_main\n",
    "\n",
    "# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\n",
    "fetch_matches_main([\n",
    "    \"--target-match-count\", \"1000\",\n",
    "    \"--matches-per-player\", \"100\",\n",
    "    \"--history-window\", \"600\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f26b3",
   "metadata": {},
   "source": [
    "## Pregame Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 16:08:14,658 [INFO] Starting with 1000 existing matches\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfetch_eune_matches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m fetch_matches_main\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mfetch_matches_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--target-match-count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--matches-per-player\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--history-window\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m600\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:241\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(argv)\u001b[39m\n\u001b[32m    238\u001b[39m history_window = \u001b[38;5;28mmax\u001b[39m(batch_size, args.history_window)\n\u001b[32m    239\u001b[39m queue_filter = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args.queue_id < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m args.queue_id\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_gold_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pages_per_division\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpuuid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpuuid\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:122\u001b[39m, in \u001b[36miter_gold_entries\u001b[39m\u001b[34m(api, max_pages)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_pages + \u001b[32m1\u001b[39m):\n\u001b[32m    121\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEAGUE_ROUTE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lol/league/v4/entries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUEUE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdivision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     entries: List[Dict] = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entries:\n\u001b[32m    124\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mNo more entries for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m page \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, division, page)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexs\\Desktop\\AI Semester 4\\Challange\\Data Processing Notebook\\scripts\\fetch_eune_matches.py:113\u001b[39m, in \u001b[36mRiotAPI.get_json\u001b[39m\u001b[34m(self, url, params, retries)\u001b[39m\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGiving up on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Request failed (401) for https://eun1.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/GOLD/I: {\"status\":{\"message\":\"Unknown apikey\",\"status_code\":401}}"
     ]
    }
   ],
   "source": [
    "from scripts.fetch_eune_matches import main as fetch_matches_main\n",
    "\n",
    "# Runs like: !python scripts/fetch_eune_matches.py --target-match-count 1000 ...\n",
    "fetch_matches_main([\n",
    "    \"--target-match-count\", \"1000\",\n",
    "    \"--matches-per-player\", \"100\",\n",
    "    \"--history-window\", \"600\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefdb45",
   "metadata": {},
   "source": [
    "## Fifteen Minute Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base match JSON files in data/raw: 1000\n",
      "Total match IDs from matchlist.json: 1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TARGET_MINUTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m     match_ids_all = json.load(f)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTotal match IDs from matchlist.json:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(match_ids_all))\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m MIN_GAME_DURATION = \u001b[43mTARGET_MINUTE\u001b[49m * \u001b[32m60\u001b[39m  \u001b[38;5;66;03m# require full duration to compute 15m stats\u001b[39;00m\n\u001b[32m     15\u001b[39m rows = []\n\u001b[32m     16\u001b[39m skipped_short = []\n",
      "\u001b[31mNameError\u001b[39m: name 'TARGET_MINUTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Build 15-Minute Table\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "all_match_files = [f.replace('.json', '') for f in os.listdir(raw_dir) if f.endswith('.json') and not f.endswith('_timeline.json')]\n",
    "print(\"Number of base match JSON files in data/raw:\", len(all_match_files))\n",
    "\n",
    "\n",
    "# Load full list from matchlist.json\n",
    "with open('matchlist.json', 'r') as f:\n",
    "    match_ids_all = json.load(f)\n",
    "print('Total match IDs from matchlist.json:', len(match_ids_all))\n",
    "\n",
    "\n",
    "MIN_GAME_DURATION = TARGET_MINUTE * 60  # require full duration to compute 15m stats\n",
    "rows = []\n",
    "skipped_short = []\n",
    "missing_timeline = []\n",
    "skipped_snapshot = []\n",
    "\n",
    "# Process each match\n",
    "for match_id in tqdm(match_ids_all, desc=\"Processing matches (15m)\"):\n",
    "    try:\n",
    "        match_json = load_match_json(match_id)\n",
    "    except FileNotFoundError:\n",
    "        continue  # skip if match file missing\n",
    "    game_duration = match_json.get('info', {}).get('gameDuration', 0)\n",
    "    if game_duration < MIN_GAME_DURATION:  # ignore super short games (remakes)\n",
    "        skipped_short.append((match_id, game_duration))\n",
    "        continue\n",
    "    try:\n",
    "        timeline_json = load_timeline_json(match_id)  # need events for early stats\n",
    "    except FileNotFoundError:\n",
    "        missing_timeline.append(match_id)\n",
    "        continue\n",
    "    try:\n",
    "        stats_snapshot = aggregate_participant_stats_at_minute(match_json, timeline_json)\n",
    "    except ValueError as exc:\n",
    "        skipped_snapshot.append((match_id, str(exc)))\n",
    "        continue\n",
    "    teams = match_json['info']['teams']\n",
    "\n",
    "\n",
    "\n",
    "# Method to determine which team got first objective\n",
    "def first_team(obj_key):\n",
    "    try:\n",
    "        if teams[0]['objectives'][obj_key]['first']:\n",
    "            return 'blue'\n",
    "        if teams[1]['objectives'][obj_key]['first']:\n",
    "            return 'red'\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# base row\n",
    "row = {\n",
    "    'matchId': match_id,\n",
    "    'queueId': match_json['info'].get('queueId'),\n",
    "    'gameDuration': game_duration,\n",
    "    'blue_win': int(match_json['info']['teams'][0]['win']),  # 1 if blue won\n",
    "    'first_tower': first_team('tower'),\n",
    "    'first_dragon': first_team('dragon'),\n",
    "    'first_herald': first_team('riftHerald'),\n",
    "    'first_grub': first_team('horde'),  # void grubs are exposed as HORDE in the API\n",
    "}\n",
    "\n",
    "# participantId lists split by side\n",
    "blue_ids = [p['participantId'] for p in match_json['info']['participants'][:5]]\n",
    "red_ids = [p['participantId'] for p in match_json['info']['participants'][5:]]\n",
    "\n",
    "# aggregate team sums at 15m\n",
    "row.update({\n",
    "    'blue_gold_15': sum(stats_snapshot[pid]['totalGold'] for pid in blue_ids),\n",
    "    'red_gold_15': sum(stats_snapshot[pid]['totalGold'] for pid in red_ids),\n",
    "    'blue_xp_15': sum(stats_snapshot[pid]['xp'] for pid in blue_ids),\n",
    "    'red_xp_15': sum(stats_snapshot[pid]['xp'] for pid in red_ids),\n",
    "    'blue_cs_15': sum(stats_snapshot[pid]['minionsKilled'] for pid in blue_ids),\n",
    "    'red_cs_15': sum(stats_snapshot[pid]['minionsKilled'] for pid in red_ids),\n",
    "    'blue_kills_15': sum(stats_snapshot[pid]['kills'] for pid in blue_ids),\n",
    "    'red_kills_15': sum(stats_snapshot[pid]['kills'] for pid in red_ids),\n",
    "})\n",
    "\n",
    "# diffs (blue - red)\n",
    "row['gold_diff_15'] = row['blue_gold_15'] - row['red_gold_15']\n",
    "row['cs_diff_15'] = row['blue_cs_15'] - row['red_cs_15']\n",
    "row['xp_diff_15'] = row['blue_xp_15'] - row['red_xp_15']\n",
    "row['kills_diff_15'] = row['blue_kills_15'] - row['red_kills_15']\n",
    "rows.append(row)\n",
    "\n",
    "# Build DataFrame\n",
    "df_15m = pd.DataFrame(rows)\n",
    "\n",
    "# Save outputs (full + small preview)\n",
    "fifteen_parquet = os.path.join(processed_dir, 'lol_15min_data.parquet')\n",
    "fifteen_csv = os.path.join(processed_dir, 'lol_15min_data_preview.csv')\n",
    "df_15m.to_parquet(fifteen_parquet, index=False)\n",
    "df_15m.head(50).to_csv(fifteen_csv, index=False)\n",
    "print('Saved 15-minute dataset files (Parquet + preview CSV).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
