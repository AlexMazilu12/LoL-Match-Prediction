{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5e2519",
   "metadata": {},
   "source": [
    "## League of Legends - Data Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0eac94",
   "metadata": {},
   "source": [
    "Purpose: This notebook documents a clear and reproducible Data Processing workflow for the League of Legends Match Prediction challange. It explains why each step is needed, the code to collect and transform Riot match data, and produces a clean dataset ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9f745",
   "metadata": {},
   "source": [
    "## Introduction & Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd1b94",
   "metadata": {},
   "source": [
    "**Goals of this notebook:**\n",
    "\n",
    "-   Collect match data from the Riot API.\n",
    "\n",
    "-   Parse and extract both pre-game features (champion picks, roles) and early-game features at 10 minutes (gold diff, kills, objectives).\n",
    "\n",
    "-   Produce a cleaned, versioned dataset (CSV) with clear documentation for each column.\n",
    "\n",
    "**Why separate processing from modeling?**\n",
    "\n",
    "Riot match timelines and match lists can become large (MBs–GBs). Processing and cleaning in a dedicated notebook or script avoids repeated heavy work during model experimentation and keeps the ML notebook focused on training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a1533",
   "metadata": {},
   "source": [
    "## Environment & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Python libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1)\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "RIOT_API_KEY = os.getenv('RIOT_API_KEY')\n",
    "if not RIOT_API_KEY:\n",
    "    raise RuntimeError('Set RIOT_API_KEY as an environment variable or in a .env file')\n",
    "\n",
    "\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103b96e",
   "metadata": {},
   "source": [
    "## Data sourcing (Riot API overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9d871",
   "metadata": {},
   "source": [
    "### What endpoints we be used:\n",
    "\n",
    "- **Match IDs** by **PUUID:** /lol/match/v5/matches/by-puuid/{puuid}/ids - returns a list of match IDs for a given player.\n",
    "\n",
    "- **Match detail by matchId**: /lol/match/v5/matches/{matchId} — returns the full match info and timeline (timelines might be included alongside).\n",
    "\n",
    "### Filtering notes:\n",
    "\n",
    "- Use **queue=420** to restrict to **Ranked Solo/Duo matches (competitive).**\n",
    "\n",
    "- Filter out **remakes** or **extremely short games** (e.g., gameDuration < 9 minutes) to avoid noisy examples.\n",
    "\n",
    "### Rate limits & best practices:\n",
    "\n",
    "- Riot enforces **rate limits**, so I will be using **time.sleep** to pause between requests and wait longer if blocked (HTTP 429).\n",
    "\n",
    "- **Cache** raw match JSONs to disk so you don't re-download them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf40fb2",
   "metadata": {},
   "source": [
    "## Data requirements and Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd5221",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Core Features (and why)\n",
    "\n",
    "- **Match metadata**: `matchId`, `queueId`, `gameDuration` - needed to filter matches.\n",
    "- **Champion & role picks**: champion IDs/names, team positions - needed for pre-game features.\n",
    "- **Lane metrics at 10 minutes**: gold diff, CS diff, XP diff, K/D/A per lane - strong early indicators.\n",
    "- **Team objectives at 10 minutes**: `firstBlood`, `firstTower`, `firstDragon` (+ type), `firstHerald`, counts of towers/dragons.\n",
    "- **Target variables**: `blue_win` (0/1) and `win_probability` (for model output).\n",
    "\n",
    "\n",
    "\n",
    "### 2. Proposed Output Schema (one row per match)\n",
    "\n",
    "### Match Metadata\n",
    "- `matchId` (`str`)\n",
    "- `queueId` (`int`)\n",
    "- `gameDuration` (`int`, seconds)\n",
    "- `blue_win` (`0/1`)\n",
    "\n",
    "### Champion Picks & Roles\n",
    "- `blue_champions` (list of champion IDs)\n",
    "- `red_champions` (list of champion IDs)\n",
    "- `blue_roles` (list of positions)\n",
    "- `red_roles` (list of positions)\n",
    "\n",
    "### Early Features (10 min)\n",
    "- `blue_gold_10`, `red_gold_10`, `gold_diff_10` - difference in gold at 10 minutes\n",
    "- `blue_cs_10`, `red_cs_10`, `cs_diff_10` - difference in farm at 10 minutes\n",
    "- `blue_xp_10`, `red_xp_10`, `xp_diff_10` - difference in xp at 10 minutes\n",
    "- `blue_kills_10`, `red_kills_10`, `kills_diff_10`- difference in kills at 10 minutes\n",
    "\n",
    "### Objectives (10 min)\n",
    "- `first_blood` (None / blue / red)\n",
    "- `first_tower` (None / blue / red)\n",
    "- `first_dragon` (None / blue / red)\n",
    "- `first_rift_herald` (None / blue / red)\n",
    "- `blue_towers_10`, `red_towers_10` - tower kills at 10 minutes\n",
    "- `blue_dragons_10`, `red_dragons_10` - dragon kills at 10 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef368c3",
   "metadata": {},
   "source": [
    "## Storage strategy & versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4e191",
   "metadata": {},
   "source": [
    "For this project, I decided to keep storage dead simple.\n",
    "### Raw and Processed Data\n",
    "- **Raw data**: every match JSON from the Riot API lives in `data/raw/` (both the match detail and its timeline). I never overwrite these so I can always reprocess.\n",
    "- **Processed data**: two Parquet files:\n",
    "  - `data/processed/lol_pregame_data.parquet` (lane champion picks)\n",
    "  - `data/processed/lol_10min_data.parquet` (early game stats + first objectives)\n",
    "- **Preview CSVs**: `lol_pregame_data_preview.csv` and `lol_10min_data_preview.csv` (just first 50 rows to eyeball).\n",
    "\n",
    "Why Parquet? Smaller + keeps types + faster to load. CSV is only for quick inspection.\n",
    "\n",
    "No manifest / no versioning right now — I just overwrite while iterating. If I later need reproducibility or experiment tracking, I can start saving versioned snapshots like `lol_pregame_data_v002.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f19a3",
   "metadata": {},
   "source": [
    "## Raw data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d4d23",
   "metadata": {},
   "source": [
    "### Important initial design choices:\n",
    "\n",
    "- I am going to collect match IDs by querying several PUUIDs (players). Prefer high-activity public accounts.\n",
    "\n",
    "- For initial proof-of-concept, I will collect 1,000–5,000 matches. For final model I will have tens of thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30755ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUUIDs fetched: ['T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading matches for T9lGIR8iroZCD7e9-3hWNs-wg9h3eA1UjcT_4YsKlkdZY0L9tZWhJlMg9kGT99wpuBNZxT5iDpY3lg: 100%|██████████| 20/20 [00:00<00:00, 2471.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded 20 matches to 'data/raw/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants + basic Riot fetch helpers (simple version)\n",
    "REGION = 'europe'  # riot regional routing\n",
    "HEADERS = {\"X-Riot-Token\": RIOT_API_KEY}  # auth header\n",
    "MATCHLIST_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/by-puuid/{{puuid}}/ids'\n",
    "MATCH_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}'\n",
    "TIMELINE_URL = f'https://{REGION}.api.riotgames.com/lol/match/v5/matches/{{matchId}}/timeline'\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Basic GET with tiny retry + rate limit handling\n",
    "def safe_get(url, params=None, retries=5):\n",
    "    for i in range(retries):\n",
    "        resp = requests.get(url, headers=HEADERS, params=params)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.json()\n",
    "        elif resp.status_code == 429:  # rate limited\n",
    "            wait = int(resp.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to GET {url} after {retries} retries\")\n",
    "\n",
    "# Get player unique id from riot name + tag\n",
    "def get_puuid(game_name, tag_line):\n",
    "    url = f\"https://{REGION}.api.riotgames.com/riot/account/v1/accounts/by-riot-id/{game_name}/{tag_line}\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json().get(\"puuid\")\n",
    "    print(f\"Failed to get PUUID for {game_name}#{tag_line}: {resp.status_code}\")\n",
    "    return None\n",
    "\n",
    "# Just grab some match ids for one player (solo queue)\n",
    "def fetch_match_ids(puuid, count=20, queue=420):\n",
    "    params = {\"start\": 0, \"count\": count, \"queue\": queue}\n",
    "    return safe_get(MATCHLIST_URL.format(puuid=puuid), params=params)\n",
    "\n",
    "# Save both match + timeline JSON locally if missing (cache)\n",
    "def fetch_and_save_match_with_timeline(match_id):\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    match_path = os.path.join(RAW_DIR, f\"{match_id}.json\")\n",
    "    if not os.path.exists(match_path):  # don't redownload\n",
    "        match_data = safe_get(MATCH_URL.format(matchId=match_id))\n",
    "        with open(match_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(match_data, f)\n",
    "\n",
    "    timeline_path = os.path.join(RAW_DIR, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(timeline_path):\n",
    "        timeline_data = safe_get(TIMELINE_URL.format(matchId=match_id))\n",
    "        with open(timeline_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(timeline_data, f)\n",
    "\n",
    "    return match_path, timeline_path\n",
    "\n",
    "# Generic fetch/save helper\n",
    "def fetch_and_save_json(url, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    if os.path.exists(path):  # already there\n",
    "        return path\n",
    "    data = safe_get(url)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    return path\n",
    "\n",
    "# Single test player (can add more later)\n",
    "players = [{\"name\": \"BB99\", \"tag\": \"BLZNT\"}]\n",
    "\n",
    "# Resolve their PUUIDs\n",
    "for p in players:\n",
    "    p[\"puuid\"] = get_puuid(p[\"name\"], p[\"tag\"])\n",
    "puuids = [p[\"puuid\"] for p in players if p.get(\"puuid\")]\n",
    "print(\"PUUIDs fetched:\", puuids)\n",
    "\n",
    "# Rate limiting guard (rough)\n",
    "all_match_ids = []\n",
    "MAX_REQUESTS = 99  # under 100 per 2 min window\n",
    "WINDOW = 120\n",
    "request_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for puuid in puuids:\n",
    "    match_ids = fetch_match_ids(puuid, count=20)  # small sample\n",
    "    all_match_ids.extend(match_ids)\n",
    "    \n",
    "    for match_id in tqdm(match_ids, desc=f\"Downloading matches for {puuid}\"):\n",
    "        # Full match\n",
    "        fetch_and_save_json(MATCH_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}.json\"))\n",
    "        # Timeline\n",
    "        fetch_and_save_json(TIMELINE_URL.format(matchId=match_id), os.path.join(RAW_DIR, f\"{match_id}_timeline.json\"))\n",
    "        \n",
    "        request_count += 2  # two calls per match\n",
    "        if request_count >= MAX_REQUESTS:\n",
    "            elapsed = time.time() - start_time\n",
    "            sleep_time = max(0, WINDOW - elapsed)\n",
    "            if sleep_time > 0:\n",
    "                print(f\"Sleeping {sleep_time:.1f}s to avoid rate limit...\")\n",
    "                time.sleep(sleep_time)\n",
    "            start_time = time.time()\n",
    "            request_count = 0\n",
    "\n",
    "# Persist the collected list\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(all_match_ids, f, indent=2)\n",
    "\n",
    "print(f\" Downloaded {len(all_match_ids)} matches to '{RAW_DIR}/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b9bfc",
   "metadata": {},
   "source": [
    "### Regenerate matchlist.json after adding new matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4431f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated matchlist.json with 45 match IDs from data/raw\n"
     ]
    }
   ],
   "source": [
    "# Regenerate matchlist.json from all match JSON files in data/raw\n",
    "import os\n",
    "import json\n",
    "\n",
    "raw_dir = 'data/raw'\n",
    "match_ids = []\n",
    "for fname in os.listdir(raw_dir):\n",
    "    if fname.endswith('.json') and not fname.endswith('_timeline.json'):\n",
    "        match_id = fname.replace('.json', '')\n",
    "        match_ids.append(match_id)\n",
    "\n",
    "# Remove duplicates, just in case\n",
    "match_ids = sorted(set(match_ids))\n",
    "\n",
    "with open(\"matchlist.json\", \"w\") as f:\n",
    "    json.dump(match_ids, f, indent=2)\n",
    "\n",
    "print(f\"Updated matchlist.json with {len(match_ids)} match IDs from {raw_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6456415",
   "metadata": {},
   "source": [
    "## Processing & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46cc98",
   "metadata": {},
   "source": [
    "### Design approach (how I actually set this up)\n",
    "\n",
    "To keep it simple the data is split into two parts based on when the state of the game:\n",
    "\n",
    "1. **Pregame table** – only info known before the game starts:\n",
    "   - Champion picks by lane (top, jg, mid, bot, support) for each team.\n",
    "   - Match duration (just for quick filtering later).\n",
    "\n",
    "2. **10-minute table** – snapshot of early game:\n",
    "   - First objective takers (`tower`, `dragon`, `herald`, grubs = `horde`).\n",
    "   - Team gold / xp / cs / kills and their diffs at ~10:00.\n",
    "   - Games shorter than 10 minutes are skipped (remakes / noise).\n",
    "\n",
    "Both tables have one row per `matchId`, so joining them later is trivial.\n",
    "\n",
    "Why bother splitting? Faster iteration. I can train a draft-only model without loading timeline data, and I can tweak early game logic without touching the pregame export.\n",
    "\n",
    "Storage:\n",
    "- Parquet main files: `lol_pregame_data.parquet` and `lol_10min_data.parquet`.\n",
    "- Small CSV previews (`*_preview.csv`) just for a quick look.\n",
    "\n",
    "No manifest, no version bumps — if I change logic I just overwrite the files (this is small scale). If I later need history I can start versioning again.\n",
    "\n",
    "Flow I run:\n",
    "1. Regenerate `matchlist.json` from raw if needed.\n",
    "2. Build pregame (no timeline needed).\n",
    "3. Build 10m table (needs timeline + duration filter).\n",
    "4. Save Parquet + preview CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7826503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup for loading previously downloaded JSON files\n",
    "\n",
    "RAW_DIR = 'data/raw'\n",
    "\n",
    "# Load a stored match detail JSON by match_id.\n",
    "def load_match_json(match_id, raw_dir: str = RAW_DIR):\n",
    "    path = os.path.join(raw_dir, f\"{match_id}.json\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Match file not found: {path}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load a stored timeline JSON by match_id.\n",
    "def load_timeline_json(match_id, raw_dir: str = RAW_DIR):\n",
    "    path = os.path.join(raw_dir, f\"{match_id}_timeline.json\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Timeline file not found: {path}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# If aggregate_participant_stats_at_10min not defined yet, create a minimal placeholder to avoid NameError.\n",
    "# Replace later with real logic if already implemented elsewhere.\n",
    "try:\n",
    "    aggregate_participant_stats_at_10min\n",
    "except NameError:\n",
    "    def aggregate_participant_stats_at_10min(match_json, timeline_json):\n",
    "        \"\"\"Placeholder aggregator: returns basic dict with zeroed stats for each participantId.\n",
    "        Replace with real implementation that inspects frames around 10:00.\n",
    "        \"\"\"\n",
    "        participants = match_json['info']['participants']\n",
    "        out = {}\n",
    "        for p in participants:\n",
    "            pid = p['participantId']\n",
    "            out[pid] = {\n",
    "                'totalGold': p.get('goldEarned', 0),  # fallback to final gold if no timeline logic\n",
    "                'xp': p.get('champExperience', 0), # fallback to final xp if no timeline logic\n",
    "                'minionsKilled': p.get('totalMinionsKilled', 0) + p.get('neutralMinionsKilled', 0), # fallback to final if no timeline logic\n",
    "                'kills': p.get('kills', 0) # fallback to final kills if no timeline logic\n",
    "            }\n",
    "        return out\n",
    "    print(\"[WARN] Using placeholder aggregate_participant_stats_at_10min. Implement proper 10m snapshot logic later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f26b3",
   "metadata": {},
   "source": [
    "## Pregame Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70fa902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pregame (all lanes): 100%|██████████| 45/45 [00:00<00:00, 1063.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified pregame table saved: data/processed\\lol_pregame_data.parquet (rows=45)\n",
      "Columns: ['matchId', 'gameDuration', 'bluetop', 'bluejg', 'bluemid', 'bluebot', 'bluesupport', 'redtop', 'redjg', 'redmid', 'redbot', 'redsupport']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build Pregame Table\n",
    "\n",
    "processed_dir = 'data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)  # make sure folder exists\n",
    "\n",
    "# Load list of match IDs downloaded\n",
    "with open('matchlist.json','r') as f:\n",
    "    match_ids_all = json.load(f)\n",
    "\n",
    "# Map Riot role labels to shorter ones\n",
    "lane_map = {\n",
    "    'TOP': 'Top',\n",
    "    'JUNGLE': 'Jg',\n",
    "    'MIDDLE': 'Mid',\n",
    "    'BOTTOM': 'Bot',\n",
    "    'UTILITY': 'Support'\n",
    "}\n",
    "\n",
    "pregame_rows = []\n",
    "\n",
    "for mid in tqdm(match_ids_all, desc='Pregame (all lanes)'):\n",
    "    m = load_match_json(mid)  # raw match json\n",
    "    info = m['info']\n",
    "    participants = info['participants']\n",
    "    # placeholders for each lane (filled in later)\n",
    "    bluetop = bluejg = bluemid = bluebot = bluesupport = ''\n",
    "    redtop = redjg = redmid = redbot = redsupport = ''\n",
    "    for p in participants:\n",
    "        pos = p.get('teamPosition') # TOP, JUNGLE, MIDDLE, BOTTOM, SUPPORT\n",
    "        team_id = p.get('teamId')\n",
    "        if pos in lane_map:\n",
    "            lane_short = lane_map[pos]\n",
    "            champ = p.get('championName')\n",
    "            # assign champion to first empty slot for that lane\n",
    "            if team_id == 100:  # blue side\n",
    "                if lane_short == 'top' and not bluetop: bluetop = champ\n",
    "                elif lane_short == 'jg' and not bluejg: bluejg = champ\n",
    "                elif lane_short == 'mid' and not bluemid: bluemid = champ\n",
    "                elif lane_short == 'bot' and not bluebot: bluebot = champ\n",
    "                elif lane_short == 'support' and not bluesupport: bluesupport = champ\n",
    "            elif team_id == 200:  # red side\n",
    "                if lane_short == 'top' and not redtop: redtop = champ\n",
    "                elif lane_short == 'jg' and not redjg: redjg = champ\n",
    "                elif lane_short == 'mid' and not redmid: redmid = champ\n",
    "                elif lane_short == 'bot' and not redbot: redbot = champ\n",
    "                elif lane_short == 'support' and not redsupport: redsupport = champ\n",
    "    # one row per match\n",
    "    row = {\n",
    "        'matchId': mid,\n",
    "        'gameDuration': info.get('gameDuration', 0),  # length in seconds\n",
    "        'bluetop': bluetop,\n",
    "        'bluejg': bluejg,\n",
    "        'bluemid': bluemid,\n",
    "        'bluebot': bluebot,\n",
    "        'bluesupport': bluesupport,\n",
    "        'redtop': redtop,\n",
    "        'redjg': redjg,\n",
    "        'redmid': redmid,\n",
    "        'redbot': redbot,\n",
    "        'redsupport': redsupport\n",
    "    }\n",
    "    pregame_rows.append(row)\n",
    "\n",
    "df_pregame = pd.DataFrame(pregame_rows)\n",
    "pregame_path_parquet = os.path.join(processed_dir, 'lol_pregame_data.parquet')\n",
    "pregame_path_csv = os.path.join(processed_dir, 'lol_pregame_data_preview.csv')\n",
    "# Save full + small preview (first 50 rows)\n",
    "df_pregame.to_parquet(pregame_path_parquet, index=False)\n",
    "df_pregame.head(50).to_csv(pregame_path_csv, index=False)\n",
    "print(f\"Simplified pregame table saved: {pregame_path_parquet} (rows={len(df_pregame)})\")\n",
    "print('Columns:', list(df_pregame.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefdb45",
   "metadata": {},
   "source": [
    "## Ten Minute Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1f5b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of match JSON files in data/raw: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing matches (10m): 100%|██████████| 20/20 [00:00<00:00, 113.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in 10m DataFrame (>= 600s): 19\n",
      "Skipped 1 short games (<600s):\n",
      "  - EUN1_3832486067 (duration=117s)\n",
      "           matchId  gameDuration first_tower first_dragon first_herald  \\\n",
      "0  EUN1_3834387321          1995        blue         blue         blue   \n",
      "1  EUN1_3834185747          2567         red         blue          red   \n",
      "2  EUN1_3834161960          1674        blue         blue         blue   \n",
      "3  EUN1_3833729445          2134        blue          red         blue   \n",
      "4  EUN1_3833574230          1906         red          red         blue   \n",
      "\n",
      "  first_grub  \n",
      "0       blue  \n",
      "1        red  \n",
      "2       blue  \n",
      "3        red  \n",
      "4        red  \n",
      "Saved 10-minute dataset files (Parquet + preview CSV).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all matches (10-minute table)\n",
    "raw_dir = 'data/raw'\n",
    "all_match_files = [f.replace('.json','') for f in os.listdir(raw_dir) if f.endswith('.json')]\n",
    "\n",
    "print(\"Number of match JSON files in data/raw:\", len([f for f in os.listdir(raw_dir) if f.endswith('.json')]))\n",
    "\n",
    "MIN_GAME_DURATION = 600  # 10 minutes minimum to include\n",
    "rows = []\n",
    "skipped_short = []\n",
    "\n",
    "for match_id in tqdm(all_match_ids, desc=\"Processing matches (10m)\"):\n",
    "    match_json = load_match_json(match_id)\n",
    "    game_duration = match_json['info'].get('gameDuration', 0)\n",
    "    if game_duration < MIN_GAME_DURATION:  # ignore super short games\n",
    "        skipped_short.append((match_id, game_duration))\n",
    "        continue\n",
    "    \n",
    "    timeline_json = load_timeline_json(match_id)  # need events for early stats\n",
    "    stats_10min = aggregate_participant_stats_at_10min(match_json, timeline_json)  # custom helper\n",
    "    teams = match_json['info']['teams']\n",
    "\n",
    "    # who got first objective\n",
    "    def first_team(obj_key):\n",
    "        try:\n",
    "            if teams[0]['objectives'][obj_key]['first']:\n",
    "                return 'blue'\n",
    "            if teams[1]['objectives'][obj_key]['first']:\n",
    "                return 'red'\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    # base row\n",
    "    row = {\n",
    "        'matchId': match_id,\n",
    "        'queueId': match_json['info'].get('queueId'),\n",
    "        'gameDuration': game_duration,\n",
    "        'blue_win': int(match_json['info']['teams'][0]['win']),  # 1 if blue won\n",
    "        'first_tower': first_team('tower'),\n",
    "        'first_dragon': first_team('dragon'),\n",
    "        'first_herald': first_team('riftHerald'),\n",
    "        'first_grub': first_team('horde')  # void grub (horde)\n",
    "    }\n",
    "    # participantId lists (1..10) split by side\n",
    "    blue_ids = [p['participantId'] for p in match_json['info']['participants'][:5]]\n",
    "    red_ids = [p['participantId'] for p in match_json['info']['participants'][5:]]\n",
    "    # aggregate team sums at 10m\n",
    "    row.update({\n",
    "        'blue_gold_10': sum(stats_10min[pid]['totalGold'] for pid in blue_ids),\n",
    "        'red_gold_10': sum(stats_10min[pid]['totalGold'] for pid in red_ids),\n",
    "        'blue_xp_10': sum(stats_10min[pid]['xp'] for pid in blue_ids),\n",
    "        'red_xp_10': sum(stats_10min[pid]['xp'] for pid in red_ids),\n",
    "        'blue_cs_10': sum(stats_10min[pid]['minionsKilled'] for pid in blue_ids),\n",
    "        'red_cs_10': sum(stats_10min[pid]['minionsKilled'] for pid in red_ids),\n",
    "        'blue_kills_10': sum(stats_10min[pid]['kills'] for pid in blue_ids),\n",
    "        'red_kills_10': sum(stats_10min[pid]['kills'] for pid in red_ids),\n",
    "    })\n",
    "    # diffs (blue - red)\n",
    "    row['gold_diff_10'] = row['blue_gold_10'] - row['red_gold_10']\n",
    "    row['cs_diff_10'] = row['blue_cs_10'] - row['red_cs_10']\n",
    "    row['xp_diff_10'] = row['blue_xp_10'] - row['red_xp_10']\n",
    "    row['kills_diff_10'] = row['blue_kills_10'] - row['red_kills_10']\n",
    "    rows.append(row)\n",
    "\n",
    "df_10m = pd.DataFrame(rows)\n",
    "print(f\"Rows in 10m DataFrame (>= {MIN_GAME_DURATION}s):\", df_10m.shape[0])\n",
    "if skipped_short:\n",
    "    print(f\"Skipped {len(skipped_short)} short games (<{MIN_GAME_DURATION}s):\")\n",
    "    for mid, dur in skipped_short[:10]:\n",
    "        print(f\"  - {mid} (duration={dur}s)\")\n",
    "    if len(skipped_short) > 10:\n",
    "        print(f\"  ... {len(skipped_short)-10} more\")\n",
    "print(df_10m[['matchId','gameDuration','first_tower','first_dragon','first_herald','first_grub']].head())\n",
    "\n",
    "# Save outputs (full + small preview)\n",
    "tenmin_parquet = os.path.join(processed_dir, 'lol_10min_data.parquet')\n",
    "tenmin_csv = os.path.join(processed_dir, 'lol_10min_data_preview.csv')\n",
    "df_10m.to_parquet(tenmin_parquet, index=False)\n",
    "df_10m.head(50).to_csv(tenmin_csv, index=False)\n",
    "print('Saved 10-minute dataset files (Parquet + preview CSV).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
